{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124d3ed4-c543-4399-8f2f-824c1bf79ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_name</th><th>lab_results</th><th>nursing_notes</th></tr></thead><tbody><tr><td>101</td><td>John Doe</td><td>Troponin: Elevated (2025-11-10)|ECG: ST-segment elevation (2025-11-10)|BNP: High (2025-11-11)</td><td>Patient reports 8/10 chest pain, radiating to left arm.|Vitals stable post-aspirin.|Family anxious, provided update.</td></tr><tr><td>102</td><td>Jane Smith</td><td>X-Ray (Right Hip): Femoral neck fracture (2025-11-09)|Hgb: 9.5 g/dL (Low) (2025-11-10)</td><td>Patient in significant pain pre-op.|Post-op: Vitals stable. Pain managed with PCA pump.|Physical therapy consult ordered for tomorrow.</td></tr><tr><td>103</td><td>Robert Brown</td><td>Sputum Culture: Pending (2025-11-11)|ABG: Respiratory acidosis (2025-11-11)|Chest X-Ray: Infiltrates in right lower lobe (2025-11-10)</td><td>Patient has persistent cough and fever (101.5 F).|On 2L nasal cannula. O2 sats at 92%.|Started on IV antibiotics (Ceftriaxone).</td></tr><tr><td>104</td><td>Maria Garcia</td><td>Blood Glucose: 450 mg/dL (High) (2025-11-11)|Urinalysis: Ketones present (2025-11-11)|A1c: 11.2% (2025-11-11)</td><td>Patient admitted with symptoms of DKA.|Started on insulin drip per protocol.|Diabetes educator consulted.</td></tr><tr><td>105</td><td>David Lee</td><td>CT Head (Non-contrast): No acute bleed (2025-11-08)|MRI Brain: Acute ischemic stroke, left MCA (2025-11-09)</td><td>Patient presents with right-sided weakness and aphasia.|NIH Stroke Scale: 12.|tPA administered in ED. Patient transferred to Neuro ICU.</td></tr><tr><td>106</td><td>Sarah Chen</td><td>Amylase: 950 U/L (High) (2025-11-10)|Lipase: 1200 U/L (High) (2025-11-10)</td><td>Admitted with acute-onset abdominal pain, nausea, and vomiting.|Patient made NPO (nothing by mouth).|Receiving IV fluids and pain management (morphine).</td></tr><tr><td>107</td><td>Michael Johnson</td><td>WBC: 14.2 k/uL (High) (2025-11-11)|Wound Culture: Pending (2025-11-11)</td><td>Post-op Day 3 (Appendectomy).|Surgical incision site is red, warm, and draining purulent fluid.|Fever of 102.0 F. Dr. notified. Started on IV Vancomycin for suspected infection.</td></tr><tr><td>108</td><td>Emily White</td><td>RSV Swab: Positive (2025-11-10)</td><td>6-month-old female with respiratory distress.|Audible wheezing and intercostal retractions.|Placed in crib with cool mist. Suctioned for secretions PRN.</td></tr><tr><td>109</td><td>Kevin Patel</td><td>Urine Drug Screen: Negative (2025-11-09)|TSH: Within normal limits (2025-11-09)</td><td>Patient admitted for acute depressive episode with suicidal ideation.|Placed on 1-to-1 observation.|Safety plan in place. Patient is cooperative with treatment.</td></tr><tr><td>110</td><td>Jessica Davis</td><td>GBS: Positive (2025-11-01)|Fetal Heart Tones: Stable (140s) (2025-11-11)</td><td>Patient is 39 weeks, admitted in active labor.|Epidural placed. Receiving Penicillin G for GBS prophylaxis.|Currently 6 cm dilated, 100% effaced.</td></tr><tr><td>201</td><td>Srinivas</td><td>GFR: 25 (null)|Creatinine: 2.8 mg/dL (null)</td><td>Patient (68 y/o M) with history of diabetes presents with new leg swelling and fatigue.|Vitals: BP 160/90. Assessment: Stage 4 Chronic Kidney Disease.|Lisinopril started for BP management.|Referral to nephrology placed.</td></tr><tr><td>202</td><td>Ranjan</td><td>Rapid Strep Test: Pending (null)</td><td>Patient (22 y/o F) presents with sore throat and fever for 2 days.|Vitals: Temp 102.5F.|Throat is erythematous with visible tonsillar exudate.|Rapid strep test collected.|Plan: Will start penicillin if strep test is positive.</td></tr><tr><td>203</td><td>Pardhu</td><td>MRI Lumbar Spine: Scheduled (null)</td><td>Patient (45 y/o M) reports sharp lower back pain radiating down right leg after lifting a heavy box.|Positive straight leg raise on right. Reports numbness in right foot.|Assessment: Suspected herniated disc with sciatica.|Ibuprofen and a muscle relaxer prescribed for pain.|Patient scheduled for lumbar spine MRI.</td></tr><tr><td>205</td><td>Ramana</td><td>Hemoglobin: 7.2 g/dL (Low) (null)|Type and Cross: Ordered (2 units PRBC) (null)</td><td>Patient (70 y/o F) reports extreme fatigue, dizziness, and shortness of breath when walking.|Patient is visibly pale. Assessment: Severe Anemia.|Lab results show critical hemoglobin of 7.2.|Admitting to hospital for 2 units packed red blood cell transfusion.</td></tr><tr><td>206</td><td>Vinuthana</td><td>null</td><td>Patient (38 y/o M) reports burning chest pain, worse after meals and at night.|Also reports waking with a sore taste.|Epigastric area is tender to palpation. Assessment: GERD.|Started on daily omeprazole trial.|Patient education provided regarding diet modifications (e.g., avoiding spicy foods).</td></tr><tr><td>207</td><td>Akanksha</td><td>null</td><td>19 y/o M with known peanut allergy ate a cookie at a party.|Presents with acute lip swelling, difficulty breathing, and audible wheezing.|Hives visible on chest and neck. Assessed as anaphylaxis.|Epinephrine 0.3mg IM administered.|Diphenhydramine given. Transferring to ER for further monitoring.</td></tr><tr><td>208</td><td>Roy</td><td>null</td><td>Patient (28 y/o F) reports 3 months of constant worry, panic attacks, and poor sleep.|States symptoms are interfering with her work.|Patient appears restless and tense during assessment. Assessment: GAD.|Referred to CBT and started on a low-dose SSRI.</td></tr><tr><td>209</td><td>Pataan</td><td>TSH: 12.5 mIU/L (High) (null)|TSH: Repeat ordered in 6 weeks (null)</td><td>Patient (42 y/o F) reports weight gain despite no change in diet, fatigue, and feeling cold.|Physical exam shows dry-skin and brittle hair.|TSH lab result is high (12.5). Diagnosis: Hypothyroidism.|Started on levothyroxine 50 mcg daily.|Follow-up TSH lab scheduled for 6 weeks.</td></tr><tr><td>210</td><td>Maddy</td><td>Ankle X-ray: Pending (null)</td><td>Patient (16 y/o M) twisted his ankle playing basketball, heard a 'pop'.|Unable to bear weight on ankle.|Significant swelling and bruising noted on lateral ankle. Suspect fracture.|RICE protocol initiated. Patient sent for ankle X-ray.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "John Doe",
         "Troponin: Elevated (2025-11-10)|ECG: ST-segment elevation (2025-11-10)|BNP: High (2025-11-11)",
         "Patient reports 8/10 chest pain, radiating to left arm.|Vitals stable post-aspirin.|Family anxious, provided update."
        ],
        [
         102,
         "Jane Smith",
         "X-Ray (Right Hip): Femoral neck fracture (2025-11-09)|Hgb: 9.5 g/dL (Low) (2025-11-10)",
         "Patient in significant pain pre-op.|Post-op: Vitals stable. Pain managed with PCA pump.|Physical therapy consult ordered for tomorrow."
        ],
        [
         103,
         "Robert Brown",
         "Sputum Culture: Pending (2025-11-11)|ABG: Respiratory acidosis (2025-11-11)|Chest X-Ray: Infiltrates in right lower lobe (2025-11-10)",
         "Patient has persistent cough and fever (101.5 F).|On 2L nasal cannula. O2 sats at 92%.|Started on IV antibiotics (Ceftriaxone)."
        ],
        [
         104,
         "Maria Garcia",
         "Blood Glucose: 450 mg/dL (High) (2025-11-11)|Urinalysis: Ketones present (2025-11-11)|A1c: 11.2% (2025-11-11)",
         "Patient admitted with symptoms of DKA.|Started on insulin drip per protocol.|Diabetes educator consulted."
        ],
        [
         105,
         "David Lee",
         "CT Head (Non-contrast): No acute bleed (2025-11-08)|MRI Brain: Acute ischemic stroke, left MCA (2025-11-09)",
         "Patient presents with right-sided weakness and aphasia.|NIH Stroke Scale: 12.|tPA administered in ED. Patient transferred to Neuro ICU."
        ],
        [
         106,
         "Sarah Chen",
         "Amylase: 950 U/L (High) (2025-11-10)|Lipase: 1200 U/L (High) (2025-11-10)",
         "Admitted with acute-onset abdominal pain, nausea, and vomiting.|Patient made NPO (nothing by mouth).|Receiving IV fluids and pain management (morphine)."
        ],
        [
         107,
         "Michael Johnson",
         "WBC: 14.2 k/uL (High) (2025-11-11)|Wound Culture: Pending (2025-11-11)",
         "Post-op Day 3 (Appendectomy).|Surgical incision site is red, warm, and draining purulent fluid.|Fever of 102.0 F. Dr. notified. Started on IV Vancomycin for suspected infection."
        ],
        [
         108,
         "Emily White",
         "RSV Swab: Positive (2025-11-10)",
         "6-month-old female with respiratory distress.|Audible wheezing and intercostal retractions.|Placed in crib with cool mist. Suctioned for secretions PRN."
        ],
        [
         109,
         "Kevin Patel",
         "Urine Drug Screen: Negative (2025-11-09)|TSH: Within normal limits (2025-11-09)",
         "Patient admitted for acute depressive episode with suicidal ideation.|Placed on 1-to-1 observation.|Safety plan in place. Patient is cooperative with treatment."
        ],
        [
         110,
         "Jessica Davis",
         "GBS: Positive (2025-11-01)|Fetal Heart Tones: Stable (140s) (2025-11-11)",
         "Patient is 39 weeks, admitted in active labor.|Epidural placed. Receiving Penicillin G for GBS prophylaxis.|Currently 6 cm dilated, 100% effaced."
        ],
        [
         201,
         "Srinivas",
         "GFR: 25 (null)|Creatinine: 2.8 mg/dL (null)",
         "Patient (68 y/o M) with history of diabetes presents with new leg swelling and fatigue.|Vitals: BP 160/90. Assessment: Stage 4 Chronic Kidney Disease.|Lisinopril started for BP management.|Referral to nephrology placed."
        ],
        [
         202,
         "Ranjan",
         "Rapid Strep Test: Pending (null)",
         "Patient (22 y/o F) presents with sore throat and fever for 2 days.|Vitals: Temp 102.5F.|Throat is erythematous with visible tonsillar exudate.|Rapid strep test collected.|Plan: Will start penicillin if strep test is positive."
        ],
        [
         203,
         "Pardhu",
         "MRI Lumbar Spine: Scheduled (null)",
         "Patient (45 y/o M) reports sharp lower back pain radiating down right leg after lifting a heavy box.|Positive straight leg raise on right. Reports numbness in right foot.|Assessment: Suspected herniated disc with sciatica.|Ibuprofen and a muscle relaxer prescribed for pain.|Patient scheduled for lumbar spine MRI."
        ],
        [
         205,
         "Ramana",
         "Hemoglobin: 7.2 g/dL (Low) (null)|Type and Cross: Ordered (2 units PRBC) (null)",
         "Patient (70 y/o F) reports extreme fatigue, dizziness, and shortness of breath when walking.|Patient is visibly pale. Assessment: Severe Anemia.|Lab results show critical hemoglobin of 7.2.|Admitting to hospital for 2 units packed red blood cell transfusion."
        ],
        [
         206,
         "Vinuthana",
         null,
         "Patient (38 y/o M) reports burning chest pain, worse after meals and at night.|Also reports waking with a sore taste.|Epigastric area is tender to palpation. Assessment: GERD.|Started on daily omeprazole trial.|Patient education provided regarding diet modifications (e.g., avoiding spicy foods)."
        ],
        [
         207,
         "Akanksha",
         null,
         "19 y/o M with known peanut allergy ate a cookie at a party.|Presents with acute lip swelling, difficulty breathing, and audible wheezing.|Hives visible on chest and neck. Assessed as anaphylaxis.|Epinephrine 0.3mg IM administered.|Diphenhydramine given. Transferring to ER for further monitoring."
        ],
        [
         208,
         "Roy",
         null,
         "Patient (28 y/o F) reports 3 months of constant worry, panic attacks, and poor sleep.|States symptoms are interfering with her work.|Patient appears restless and tense during assessment. Assessment: GAD.|Referred to CBT and started on a low-dose SSRI."
        ],
        [
         209,
         "Pataan",
         "TSH: 12.5 mIU/L (High) (null)|TSH: Repeat ordered in 6 weeks (null)",
         "Patient (42 y/o F) reports weight gain despite no change in diet, fatigue, and feeling cold.|Physical exam shows dry-skin and brittle hair.|TSH lab result is high (12.5). Diagnosis: Hypothyroidism.|Started on levothyroxine 50 mcg daily.|Follow-up TSH lab scheduled for 6 weeks."
        ],
        [
         210,
         "Maddy",
         "Ankle X-ray: Pending (null)",
         "Patient (16 y/o M) twisted his ankle playing basketball, heard a 'pop'.|Unable to bear weight on ankle.|Significant swelling and bruising noted on lateral ankle. Suspect fracture.|RICE protocol initiated. Patient sent for ankle X-ray."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lab_results",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "nursing_notes",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=spark.table('patient_notes')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43ff557-2a80-4810-9695-b44815950fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\nCollecting librosa\n  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\nCollecting torch\n  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting soundfile\n  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.18.0)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.1.3)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.2)\nCollecting regex!=2019.12.17 (from transformers)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting safetensors>=0.4.3 (from transformers)\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nCollecting tqdm>=4.27 (from transformers)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting audioread>=2.1.9 (from librosa)\n  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting numba>=0.51.0 (from librosa)\n  Downloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from librosa) (1.15.1)\nRequirement already satisfied: scikit-learn>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from librosa) (1.6.1)\nRequirement already satisfied: joblib>=1.0 in /databricks/python3/lib/python3.12/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /databricks/python3/lib/python3.12/site-packages (from librosa) (5.1.1)\nCollecting pooch>=1.1 (from librosa)\n  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\nCollecting soxr>=0.3.2 (from librosa)\n  Downloading soxr-1.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: typing_extensions>=4.1.1 in /databricks/python3/lib/python3.12/site-packages (from librosa) (4.12.2)\nCollecting lazy_loader>=0.1 (from librosa)\n  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\nCollecting msgpack>=1.0 (from librosa)\n  Downloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting networkx>=2.5.1 (from torch)\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.5)\nRequirement already satisfied: fsspec>=0.8.5 in /databricks/python3/lib/python3.12/site-packages (from torch) (2023.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.5.0 (from torch)\n  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: cffi>=1.0 in /databricks/python3/lib/python3.12/site-packages (from soundfile) (1.17.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.21)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n  Downloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: platformdirs>=2.5.0 in /databricks/python3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (3.10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/12.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/12.0 MB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m9.7/12.0 MB\u001B[0m \u001B[31m25.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.0/12.0 MB\u001B[0m \u001B[31m27.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\nDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/899.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.4/899.7 MB\u001B[0m \u001B[31m101.2 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.9/899.7 MB\u001B[0m \u001B[31m101.5 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.7/899.7 MB\u001B[0m \u001B[31m100.5 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.4/899.7 MB\u001B[0m \u001B[31m64.8 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.4/899.7 MB\u001B[0m \u001B[31m71.4 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m91.8/899.7 MB\u001B[0m \u001B[31m76.8 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.4/899.7 MB\u001B[0m \u001B[31m77.9 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.5/899.7 MB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m149.4/899.7 MB\u001B[0m \u001B[31m82.0 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m165.2/899.7 MB\u001B[0m \u001B[31m81.6 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.7/899.7 MB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m186.6/899.7 MB\u001B[0m \u001B[31m76.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m197.1/899.7 MB\u001B[0m \u001B[31m75.0 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m208.7/899.7 MB\u001B[0m \u001B[31m73.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m219.2/899.7 MB\u001B[0m \u001B[31m72.2 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m229.1/899.7 MB\u001B[0m \u001B[31m70.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.1/899.7 MB\u001B[0m \u001B[31m69.5 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m248.0/899.7 MB\u001B[0m \u001B[31m68.1 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m257.7/899.7 MB\u001B[0m \u001B[31m66.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.4/899.7 MB\u001B[0m \u001B[31m65.7 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m277.1/899.7 MB\u001B[0m \u001B[31m63.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m287.6/899.7 MB\u001B[0m \u001B[31m62.3 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m298.6/899.7 MB\u001B[0m \u001B[31m60.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m309.6/899.7 MB\u001B[0m \u001B[31m59.7 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m319.0/899.7 MB\u001B[0m \u001B[31m62.3 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m327.7/899.7 MB\u001B[0m \u001B[31m60.6 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m336.3/899.7 MB\u001B[0m \u001B[31m59.0 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m345.0/899.7 MB\u001B[0m \u001B[31m57.6 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m346.3/899.7 MB\u001B[0m \u001B[31m57.3 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m350.7/899.7 MB\u001B[0m \u001B[31m53.4 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m359.9/899.7 MB\u001B[0m \u001B[31m52.5 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m367.0/899.7 MB\u001B[0m \u001B[31m51.2 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m374.3/899.7 MB\u001B[0m \u001B[31m50.0 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m381.7/899.7 MB\u001B[0m \u001B[31m48.7 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m387.2/899.7 MB\u001B[0m \u001B[31m47.7 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m393.0/899.7 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m399.0/899.7 MB\u001B[0m \u001B[31m45.2 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m405.3/899.7 MB\u001B[0m \u001B[31m44.1 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m411.6/899.7 MB\u001B[0m \u001B[31m43.2 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m418.4/899.7 MB\u001B[0m \u001B[31m42.4 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m425.7/899.7 MB\u001B[0m \u001B[31m41.7 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m433.3/899.7 MB\u001B[0m \u001B[31m41.1 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m441.5/899.7 MB\u001B[0m \u001B[31m40.8 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m449.6/899.7 MB\u001B[0m \u001B[31m40.6 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m458.0/899.7 MB\u001B[0m \u001B[31m40.3 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m466.6/899.7 MB\u001B[0m \u001B[31m40.0 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m475.8/899.7 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m485.0/899.7 MB\u001B[0m \u001B[31m39.7 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m493.6/899.7 MB\u001B[0m \u001B[31m39.5 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m503.3/899.7 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m513.5/899.7 MB\u001B[0m \u001B[31m39.7 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m524.6/899.7 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.1/899.7 MB\u001B[0m \u001B[31m40.1 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m544.5/899.7 MB\u001B[0m \u001B[31m40.2 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m555.0/899.7 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m566.2/899.7 MB\u001B[0m \u001B[31m40.0 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m578.0/899.7 MB\u001B[0m \u001B[31m40.1 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m590.9/899.7 MB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m603.5/899.7 MB\u001B[0m \u001B[31m41.3 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m616.8/899.7 MB\u001B[0m \u001B[31m43.9 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m630.7/899.7 MB\u001B[0m \u001B[31m45.0 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m641.7/899.7 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m651.4/899.7 MB\u001B[0m \u001B[31m47.0 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m656.9/899.7 MB\u001B[0m \u001B[31m46.8 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m661.9/899.7 MB\u001B[0m \u001B[31m46.6 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m667.4/899.7 MB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m673.4/899.7 MB\u001B[0m \u001B[31m46.2 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m679.7/899.7 MB\u001B[0m \u001B[31m46.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m686.3/899.7 MB\u001B[0m \u001B[31m46.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m693.1/899.7 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m700.2/899.7 MB\u001B[0m \u001B[31m45.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m704.6/899.7 MB\u001B[0m \u001B[31m45.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m708.6/899.7 MB\u001B[0m \u001B[31m44.2 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m713.0/899.7 MB\u001B[0m \u001B[31m43.4 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m717.5/899.7 MB\u001B[0m \u001B[31m42.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m721.7/899.7 MB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m725.6/899.7 MB\u001B[0m \u001B[31m41.3 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m729.5/899.7 MB\u001B[0m \u001B[31m40.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m733.7/899.7 MB\u001B[0m \u001B[31m40.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m738.5/899.7 MB\u001B[0m \u001B[31m39.5 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m743.4/899.7 MB\u001B[0m \u001B[31m38.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m748.4/899.7 MB\u001B[0m \u001B[31m38.3 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m753.7/899.7 MB\u001B[0m \u001B[31m37.8 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m759.7/899.7 MB\u001B[0m \u001B[31m37.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m765.7/899.7 MB\u001B[0m \u001B[31m37.1 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m772.5/899.7 MB\u001B[0m \u001B[31m36.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m779.4/899.7 MB\u001B[0m \u001B[31m36.3 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m785.6/899.7 MB\u001B[0m \u001B[31m35.9 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m791.2/899.7 MB\u001B[0m \u001B[31m35.4 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m796.7/899.7 MB\u001B[0m \u001B[31m34.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m801.1/899.7 MB\u001B[0m \u001B[31m34.4 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m805.3/899.7 MB\u001B[0m \u001B[31m33.8 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m810.3/899.7 MB\u001B[0m \u001B[31m33.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m815.3/899.7 MB\u001B[0m \u001B[31m33.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m820.5/899.7 MB\u001B[0m \u001B[31m32.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m826.3/899.7 MB\u001B[0m \u001B[31m32.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m832.3/899.7 MB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m838.9/899.7 MB\u001B[0m \u001B[31m31.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m844.6/899.7 MB\u001B[0m \u001B[31m31.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m849.3/899.7 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m854.6/899.7 MB\u001B[0m \u001B[31m30.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m860.4/899.7 MB\u001B[0m \u001B[31m29.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m865.9/899.7 MB\u001B[0m \u001B[31m29.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━\u001B[0m \u001B[32m871.9/899.7 MB\u001B[0m \u001B[31m29.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m878.2/899.7 MB\u001B[0m \u001B[31m28.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m885.0/899.7 MB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m891.8/899.7 MB\u001B[0m \u001B[31m28.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m898.9/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m24.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/594.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/594.3 MB\u001B[0m \u001B[31m37.7 MB/s\u001B[0m eta \u001B[36m0:00:16\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.7/594.3 MB\u001B[0m \u001B[31m38.3 MB/s\u001B[0m eta \u001B[36m0:00:16\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.4/594.3 MB\u001B[0m \u001B[31m39.8 MB/s\u001B[0m eta \u001B[36m0:00:15\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m32.8/594.3 MB\u001B[0m \u001B[31m40.3 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.9/594.3 MB\u001B[0m \u001B[31m41.3 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.4/594.3 MB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.3/594.3 MB\u001B[0m \u001B[31m43.2 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.5/594.3 MB\u001B[0m \u001B[31m42.8 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.4/594.3 MB\u001B[0m \u001B[31m43.6 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.2/594.3 MB\u001B[0m \u001B[31m44.6 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m101.4/594.3 MB\u001B[0m \u001B[31m45.6 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m111.1/594.3 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.7/594.3 MB\u001B[0m \u001B[31m46.7 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.7/594.3 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m146.8/594.3 MB\u001B[0m \u001B[31m48.4 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/594.3 MB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.8/594.3 MB\u001B[0m \u001B[31m48.8 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m180.9/594.3 MB\u001B[0m \u001B[31m49.7 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.2/594.3 MB\u001B[0m \u001B[31m50.6 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m208.1/594.3 MB\u001B[0m \u001B[31m51.5 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m222.3/594.3 MB\u001B[0m \u001B[31m52.4 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m236.2/594.3 MB\u001B[0m \u001B[31m53.1 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m251.4/594.3 MB\u001B[0m \u001B[31m54.0 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m262.7/594.3 MB\u001B[0m \u001B[31m54.2 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m270.0/594.3 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m278.1/594.3 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m286.5/594.3 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m294.1/594.3 MB\u001B[0m \u001B[31m53.9 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.4/594.3 MB\u001B[0m \u001B[31m53.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m307.0/594.3 MB\u001B[0m \u001B[31m52.6 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m313.8/594.3 MB\u001B[0m \u001B[31m52.0 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m320.9/594.3 MB\u001B[0m \u001B[31m51.4 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m327.4/594.3 MB\u001B[0m \u001B[31m51.1 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m331.1/594.3 MB\u001B[0m \u001B[31m49.9 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m335.3/594.3 MB\u001B[0m \u001B[31m48.7 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m338.2/594.3 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.6/594.3 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m345.2/594.3 MB\u001B[0m \u001B[31m45.4 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.4/594.3 MB\u001B[0m \u001B[31m44.4 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m353.9/594.3 MB\u001B[0m \u001B[31m43.5 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m358.9/594.3 MB\u001B[0m \u001B[31m42.7 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.9/594.3 MB\u001B[0m \u001B[31m42.0 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m369.4/594.3 MB\u001B[0m \u001B[31m41.4 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m374.3/594.3 MB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m378.5/594.3 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m383.0/594.3 MB\u001B[0m \u001B[31m39.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m388.2/594.3 MB\u001B[0m \u001B[31m38.5 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m393.0/594.3 MB\u001B[0m \u001B[31m37.8 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m398.2/594.3 MB\u001B[0m \u001B[31m37.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m403.7/594.3 MB\u001B[0m \u001B[31m36.6 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m410.0/594.3 MB\u001B[0m \u001B[31m36.5 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m416.3/594.3 MB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m423.1/594.3 MB\u001B[0m \u001B[31m35.5 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m429.9/594.3 MB\u001B[0m \u001B[31m35.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m435.2/594.3 MB\u001B[0m \u001B[31m34.5 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m440.7/594.3 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m446.2/594.3 MB\u001B[0m \u001B[31m33.5 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m452.2/594.3 MB\u001B[0m \u001B[31m33.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m458.8/594.3 MB\u001B[0m \u001B[31m32.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m465.3/594.3 MB\u001B[0m \u001B[31m32.2 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m470.3/594.3 MB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m475.8/594.3 MB\u001B[0m \u001B[31m31.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m480.0/594.3 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m484.2/594.3 MB\u001B[0m \u001B[31m30.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m488.1/594.3 MB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m493.1/594.3 MB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m493.6/594.3 MB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m496.0/594.3 MB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m501.2/594.3 MB\u001B[0m \u001B[31m27.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m507.0/594.3 MB\u001B[0m \u001B[31m27.4 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m513.0/594.3 MB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m519.6/594.3 MB\u001B[0m \u001B[31m26.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m526.1/594.3 MB\u001B[0m \u001B[31m26.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m533.5/594.3 MB\u001B[0m \u001B[31m26.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m540.5/594.3 MB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m548.1/594.3 MB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m556.0/594.3 MB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m564.4/594.3 MB\u001B[0m \u001B[31m26.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━\u001B[0m \u001B[32m572.8/594.3 MB\u001B[0m \u001B[31m26.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m581.7/594.3 MB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m591.1/594.3 MB\u001B[0m \u001B[31m27.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/10.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m8.9/10.2 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/88.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.1/88.0 MB\u001B[0m \u001B[31m35.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.4/88.0 MB\u001B[0m \u001B[31m35.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.0/88.0 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.1/88.0 MB\u001B[0m \u001B[31m37.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m38.8/88.0 MB\u001B[0m \u001B[31m38.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.9/88.0 MB\u001B[0m \u001B[31m38.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/88.0 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m65.0/88.0 MB\u001B[0m \u001B[31m40.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m75.0/88.0 MB\u001B[0m \u001B[31m41.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m84.7/88.0 MB\u001B[0m \u001B[31m42.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m87.8/88.0 MB\u001B[0m \u001B[31m42.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.0/88.0 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/954.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m954.8/954.8 kB\u001B[0m \u001B[31m21.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/706.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/706.8 MB\u001B[0m \u001B[31m50.2 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.7/706.8 MB\u001B[0m \u001B[31m51.3 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m31.5/706.8 MB\u001B[0m \u001B[31m52.2 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.0/706.8 MB\u001B[0m \u001B[31m53.3 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.5/706.8 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:13\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.3/706.8 MB\u001B[0m \u001B[31m54.8 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.9/706.8 MB\u001B[0m \u001B[31m56.0 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.4/706.8 MB\u001B[0m \u001B[31m56.1 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.0/706.8 MB\u001B[0m \u001B[31m56.2 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m114.8/706.8 MB\u001B[0m \u001B[31m57.0 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.9/706.8 MB\u001B[0m \u001B[31m57.2 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m140.8/706.8 MB\u001B[0m \u001B[31m58.2 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.9/706.8 MB\u001B[0m \u001B[31m59.1 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m165.9/706.8 MB\u001B[0m \u001B[31m58.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m175.9/706.8 MB\u001B[0m \u001B[31m58.1 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m186.6/706.8 MB\u001B[0m \u001B[31m57.9 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m197.1/706.8 MB\u001B[0m \u001B[31m57.5 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m205.8/706.8 MB\u001B[0m \u001B[31m56.7 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m208.9/706.8 MB\u001B[0m \u001B[31m54.6 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m217.3/706.8 MB\u001B[0m \u001B[31m53.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m223.6/706.8 MB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m229.9/706.8 MB\u001B[0m \u001B[31m51.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m237.0/706.8 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m243.8/706.8 MB\u001B[0m \u001B[31m50.4 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m251.1/706.8 MB\u001B[0m \u001B[31m49.8 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m259.0/706.8 MB\u001B[0m \u001B[31m49.3 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.1/706.8 MB\u001B[0m \u001B[31m49.0 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m275.3/706.8 MB\u001B[0m \u001B[31m48.7 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m283.9/706.8 MB\u001B[0m \u001B[31m48.3 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m292.6/706.8 MB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m302.0/706.8 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m311.2/706.8 MB\u001B[0m \u001B[31m47.3 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m321.1/706.8 MB\u001B[0m \u001B[31m47.0 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m331.4/706.8 MB\u001B[0m \u001B[31m46.7 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.8/706.8 MB\u001B[0m \u001B[31m46.5 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m352.6/706.8 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.9/706.8 MB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m375.1/706.8 MB\u001B[0m \u001B[31m46.1 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m386.9/706.8 MB\u001B[0m \u001B[31m46.1 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m399.5/706.8 MB\u001B[0m \u001B[31m45.9 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m412.1/706.8 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m425.2/706.8 MB\u001B[0m \u001B[31m45.9 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m435.2/706.8 MB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m447.7/706.8 MB\u001B[0m \u001B[31m46.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m461.6/706.8 MB\u001B[0m \u001B[31m46.7 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m475.8/706.8 MB\u001B[0m \u001B[31m49.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m490.2/706.8 MB\u001B[0m \u001B[31m51.3 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m505.2/706.8 MB\u001B[0m \u001B[31m53.8 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m520.1/706.8 MB\u001B[0m \u001B[31m56.1 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m534.8/706.8 MB\u001B[0m \u001B[31m58.0 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m546.3/706.8 MB\u001B[0m \u001B[31m59.0 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m557.8/706.8 MB\u001B[0m \u001B[31m59.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m568.9/706.8 MB\u001B[0m \u001B[31m60.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m581.2/706.8 MB\u001B[0m \u001B[31m61.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m593.8/706.8 MB\u001B[0m \u001B[31m61.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m602.4/706.8 MB\u001B[0m \u001B[31m61.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m611.1/706.8 MB\u001B[0m \u001B[31m60.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m620.5/706.8 MB\u001B[0m \u001B[31m60.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m630.5/706.8 MB\u001B[0m \u001B[31m60.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m640.4/706.8 MB\u001B[0m \u001B[31m59.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m647.8/706.8 MB\u001B[0m \u001B[31m58.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m657.7/706.8 MB\u001B[0m \u001B[31m58.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m668.5/706.8 MB\u001B[0m \u001B[31m57.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m679.7/706.8 MB\u001B[0m \u001B[31m57.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m691.0/706.8 MB\u001B[0m \u001B[31m57.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m702.0/706.8 MB\u001B[0m \u001B[31m57.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m706.7/706.8 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m706.8/706.8 MB\u001B[0m \u001B[31m45.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/193.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/193.1 MB\u001B[0m \u001B[31m32.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/193.1 MB\u001B[0m \u001B[31m31.8 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.7/193.1 MB\u001B[0m \u001B[31m32.7 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/193.1 MB\u001B[0m \u001B[31m33.2 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.8/193.1 MB\u001B[0m \u001B[31m33.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.2/193.1 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.0/193.1 MB\u001B[0m \u001B[31m34.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/193.1 MB\u001B[0m \u001B[31m35.7 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.8/193.1 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m74.4/193.1 MB\u001B[0m \u001B[31m36.9 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.6/193.1 MB\u001B[0m \u001B[31m37.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.1/193.1 MB\u001B[0m \u001B[31m38.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.0/193.1 MB\u001B[0m \u001B[31m39.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m113.0/193.1 MB\u001B[0m \u001B[31m40.0 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m123.2/193.1 MB\u001B[0m \u001B[31m40.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m134.2/193.1 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m145.5/193.1 MB\u001B[0m \u001B[31m42.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m157.0/193.1 MB\u001B[0m \u001B[31m43.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m168.6/193.1 MB\u001B[0m \u001B[31m44.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m180.4/193.1 MB\u001B[0m \u001B[31m44.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m192.2/193.1 MB\u001B[0m \u001B[31m45.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m192.9/193.1 MB\u001B[0m \u001B[31m45.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.1/193.1 MB\u001B[0m \u001B[31m41.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/63.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.1/63.6 MB\u001B[0m \u001B[31m60.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/63.6 MB\u001B[0m \u001B[31m60.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.1/63.6 MB\u001B[0m \u001B[31m56.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m43.3/63.6 MB\u001B[0m \u001B[31m53.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m53.5/63.6 MB\u001B[0m \u001B[31m52.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m63.4/63.6 MB\u001B[0m \u001B[31m52.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.6/63.6 MB\u001B[0m \u001B[31m48.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/267.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.7/267.5 MB\u001B[0m \u001B[31m52.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.8/267.5 MB\u001B[0m \u001B[31m54.3 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.3/267.5 MB\u001B[0m \u001B[31m54.9 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.7/267.5 MB\u001B[0m \u001B[31m51.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/267.5 MB\u001B[0m \u001B[31m49.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.1/267.5 MB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.4/267.5 MB\u001B[0m \u001B[31m44.4 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.5/267.5 MB\u001B[0m \u001B[31m43.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.5/267.5 MB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.9/267.5 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m91.2/267.5 MB\u001B[0m \u001B[31m41.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/267.5 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m107.5/267.5 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.9/267.5 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.0/267.5 MB\u001B[0m \u001B[31m41.0 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.5/267.5 MB\u001B[0m \u001B[31m40.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m135.5/267.5 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m142.3/267.5 MB\u001B[0m \u001B[31m39.3 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m146.8/267.5 MB\u001B[0m \u001B[31m38.4 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m151.5/267.5 MB\u001B[0m \u001B[31m37.6 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m157.0/267.5 MB\u001B[0m \u001B[31m37.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.1/267.5 MB\u001B[0m \u001B[31m36.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m169.3/267.5 MB\u001B[0m \u001B[31m36.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m175.9/267.5 MB\u001B[0m \u001B[31m36.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m182.7/267.5 MB\u001B[0m \u001B[31m36.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m188.0/267.5 MB\u001B[0m \u001B[31m35.8 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m193.2/267.5 MB\u001B[0m \u001B[31m35.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m197.1/267.5 MB\u001B[0m \u001B[31m34.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m201.3/267.5 MB\u001B[0m \u001B[31m34.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m206.0/267.5 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m211.0/267.5 MB\u001B[0m \u001B[31m33.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m216.0/267.5 MB\u001B[0m \u001B[31m33.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m221.0/267.5 MB\u001B[0m \u001B[31m33.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m225.2/267.5 MB\u001B[0m \u001B[31m32.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m229.6/267.5 MB\u001B[0m \u001B[31m32.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m234.4/267.5 MB\u001B[0m \u001B[31m32.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m239.6/267.5 MB\u001B[0m \u001B[31m32.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m244.8/267.5 MB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m250.6/267.5 MB\u001B[0m \u001B[31m31.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m256.6/267.5 MB\u001B[0m \u001B[31m31.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m262.9/267.5 MB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m267.4/267.5 MB\u001B[0m \u001B[31m31.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m267.4/267.5 MB\u001B[0m \u001B[31m31.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m267.4/267.5 MB\u001B[0m \u001B[31m31.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.5/267.5 MB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/288.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/288.2 MB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/288.2 MB\u001B[0m \u001B[31m27.4 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.5/288.2 MB\u001B[0m \u001B[31m27.1 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.3/288.2 MB\u001B[0m \u001B[31m27.4 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m28.3/288.2 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:10\u001B[0m\r\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.6/288.2 MB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.2/288.2 MB\u001B[0m \u001B[31m29.0 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m48.2/288.2 MB\u001B[0m \u001B[31m29.8 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.6/288.2 MB\u001B[0m \u001B[31m30.5 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.9/288.2 MB\u001B[0m \u001B[31m31.1 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.0/288.2 MB\u001B[0m \u001B[31m31.9 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.4/288.2 MB\u001B[0m \u001B[31m32.8 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.1/288.2 MB\u001B[0m \u001B[31m33.5 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.2/288.2 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.4/288.2 MB\u001B[0m \u001B[31m34.8 MB/s\u001B[0m eta \u001B[36m0:00:06\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.1/288.2 MB\u001B[0m \u001B[31m35.6 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.0/288.2 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m135.0/288.2 MB\u001B[0m \u001B[31m37.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m145.2/288.2 MB\u001B[0m \u001B[31m37.9 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m155.7/288.2 MB\u001B[0m \u001B[31m38.6 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.0/288.2 MB\u001B[0m \u001B[31m39.4 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m178.5/288.2 MB\u001B[0m \u001B[31m40.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m188.0/288.2 MB\u001B[0m \u001B[31m40.4 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m200.0/288.2 MB\u001B[0m \u001B[31m41.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m211.6/288.2 MB\u001B[0m \u001B[31m42.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m222.8/288.2 MB\u001B[0m \u001B[31m42.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m235.9/288.2 MB\u001B[0m \u001B[31m43.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m249.3/288.2 MB\u001B[0m \u001B[31m44.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m263.2/288.2 MB\u001B[0m \u001B[31m45.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m276.8/288.2 MB\u001B[0m \u001B[31m47.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m288.1/288.2 MB\u001B[0m \u001B[31m49.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m288.1/288.2 MB\u001B[0m \u001B[31m49.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m288.1/288.2 MB\u001B[0m \u001B[31m49.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m288.1/288.2 MB\u001B[0m \u001B[31m49.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m288.2/288.2 MB\u001B[0m \u001B[31m43.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/287.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.6/287.2 MB\u001B[0m \u001B[31m68.0 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m28.6/287.2 MB\u001B[0m \u001B[31m70.9 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.5/287.2 MB\u001B[0m \u001B[31m72.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.7/287.2 MB\u001B[0m \u001B[31m73.0 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m74.2/287.2 MB\u001B[0m \u001B[31m73.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.7/287.2 MB\u001B[0m \u001B[31m75.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m107.7/287.2 MB\u001B[0m \u001B[31m76.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m120.3/287.2 MB\u001B[0m \u001B[31m74.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m129.5/287.2 MB\u001B[0m \u001B[31m73.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m141.8/287.2 MB\u001B[0m \u001B[31m70.4 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.4/287.2 MB\u001B[0m \u001B[31m69.6 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.5/287.2 MB\u001B[0m \u001B[31m69.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m180.9/287.2 MB\u001B[0m \u001B[31m69.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m194.8/287.2 MB\u001B[0m \u001B[31m69.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m208.7/287.2 MB\u001B[0m \u001B[31m68.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m222.3/287.2 MB\u001B[0m \u001B[31m68.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m237.0/287.2 MB\u001B[0m \u001B[31m69.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m252.2/287.2 MB\u001B[0m \u001B[31m69.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m266.3/287.2 MB\u001B[0m \u001B[31m69.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m281.8/287.2 MB\u001B[0m \u001B[31m69.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m287.0/287.2 MB\u001B[0m \u001B[31m69.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m287.0/287.2 MB\u001B[0m \u001B[31m69.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m287.0/287.2 MB\u001B[0m \u001B[31m69.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m287.2/287.2 MB\u001B[0m \u001B[31m60.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/322.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.6/322.3 MB\u001B[0m \u001B[31m68.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.6/322.3 MB\u001B[0m \u001B[31m73.9 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.7/322.3 MB\u001B[0m \u001B[31m77.1 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.4/322.3 MB\u001B[0m \u001B[31m77.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.7/322.3 MB\u001B[0m \u001B[31m79.1 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.7/322.3 MB\u001B[0m \u001B[31m79.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.6/322.3 MB\u001B[0m \u001B[31m77.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.6/322.3 MB\u001B[0m \u001B[31m78.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m145.0/322.3 MB\u001B[0m \u001B[31m79.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.6/322.3 MB\u001B[0m \u001B[31m81.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m183.0/322.3 MB\u001B[0m \u001B[31m82.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/322.3 MB\u001B[0m \u001B[31m83.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m217.8/322.3 MB\u001B[0m \u001B[31m83.0 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m237.5/322.3 MB\u001B[0m \u001B[31m84.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m253.5/322.3 MB\u001B[0m \u001B[31m83.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m272.1/322.3 MB\u001B[0m \u001B[31m84.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m293.1/322.3 MB\u001B[0m \u001B[31m86.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m314.3/322.3 MB\u001B[0m \u001B[31m88.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m322.2/322.3 MB\u001B[0m \u001B[31m88.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m322.2/322.3 MB\u001B[0m \u001B[31m88.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m322.2/322.3 MB\u001B[0m \u001B[31m88.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.3/322.3 MB\u001B[0m \u001B[31m73.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/39.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.8/39.3 MB\u001B[0m \u001B[31m109.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m39.1/39.3 MB\u001B[0m \u001B[31m101.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.3/39.3 MB\u001B[0m \u001B[31m85.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/124.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.0/124.7 MB\u001B[0m \u001B[31m104.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.3/124.7 MB\u001B[0m \u001B[31m107.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.3/124.7 MB\u001B[0m \u001B[31m109.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m89.9/124.7 MB\u001B[0m \u001B[31m111.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m111.4/124.7 MB\u001B[0m \u001B[31m110.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m124.5/124.7 MB\u001B[0m \u001B[31m111.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.7/124.7 MB\u001B[0m \u001B[31m88.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\nDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/170.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.4/170.5 MB\u001B[0m \u001B[31m121.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.3/170.5 MB\u001B[0m \u001B[31m122.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.4/170.5 MB\u001B[0m \u001B[31m121.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.8/170.5 MB\u001B[0m \u001B[31m121.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m122.9/170.5 MB\u001B[0m \u001B[31m122.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m145.5/170.5 MB\u001B[0m \u001B[31m120.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m170.1/170.5 MB\u001B[0m \u001B[31m120.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m170.4/170.5 MB\u001B[0m \u001B[31m120.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.5/170.5 MB\u001B[0m \u001B[31m99.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m43.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading audioread-3.1.0-py3-none-any.whl (23 kB)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/566.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m566.1/566.1 kB\u001B[0m \u001B[31m25.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nDownloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (427 kB)\nDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m65.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.8/3.8 MB\u001B[0m \u001B[31m80.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\nDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/803.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m803.5/803.5 kB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\nDownloading soxr-1.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (238 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m60.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/56.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/56.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.1/56.3 MB\u001B[0m \u001B[31m57.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m36.4/56.3 MB\u001B[0m \u001B[31m60.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m50.6/56.3 MB\u001B[0m \u001B[31m62.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m56.1/56.3 MB\u001B[0m \u001B[31m63.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m54.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/536.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, tqdm, sympy, soxr, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, msgpack, llvmlite, lazy_loader, hf-xet, audioread, soundfile, pooch, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, huggingface-hub, tokenizers, nvidia-cusolver-cu12, librosa, transformers, torch\nSuccessfully installed audioread-3.1.0 hf-xet-1.2.0 huggingface-hub-0.36.0 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.45.1 mpmath-1.3.0 msgpack-1.1.2 networkx-3.5 numba-0.62.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pooch-1.8.2 regex-2025.11.3 safetensors-0.6.2 soundfile-0.13.1 soxr-1.0.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1 triton-3.5.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries first (run this in a separate cell)\n",
    "%pip install transformers librosa torch soundfile\n",
    "\n",
    "# Restart Python kernel to use newly installed packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a789d222-ac3d-4547-a6ee-f4b393faef6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a42b1ba-6090-41de-b401-f4d1121d32f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model for transcription...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d06ae2758df4eb69ef21eda9bdf69c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc64154897d4ca880d931e7b618331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9101dd1e304f14b4806372df376a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163e7dd5ee434c4782365229f8aedfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b91a79409745b5ab0b721de67a53ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ddedaf82874ffb838cf2174e457254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0862e6a434b2404bae740410168a816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a54bc3c6fc4060a5a8361910ece1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e034de7d312480186b423bc39980cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b48700161c54227a4ec4c1ac1c52502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2887be0a3bd249d7b0986d34e64434ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 audio files. Starting transcription...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\nUsing custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\nTranscription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Transcribed Patient ID: 101\n  > Transcribed Patient ID: 102\n  > Transcribed Patient ID: 103\n  > Transcribed Patient ID: 104\n  > Transcribed Patient ID: 105\n  > Transcribed Patient ID: 106\n  > Transcribed Patient ID: 107\n  > Transcribed Patient ID: 108\n  > Transcribed Patient ID: 109\n  > Transcribed Patient ID: 110\n  > Transcribed Patient ID: 111\n  > Transcribed Patient ID: 112\n  > Transcribed Patient ID: 113\n  > Transcribed Patient ID: 114\n  > Transcribed Patient ID: 115\n  > Transcribed Patient ID: 116\n  > Transcribed Patient ID: 117\n  > Transcribed Patient ID: 118\n  > Transcribed Patient ID: 119\n  > Transcribed Patient ID: 120\n\n--- Audio Transcripts DataFrame ---\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|patient_id|transcript_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|101       | Michael 107, subject to post-operation day 3, appendectomy, reports worsening incision pain and fever, objective fever of 102, white count 14.2, incision site is red, warm and draining fluid, assessment, surgical site infection, plan send wood culture, start bankomacinal, consult surgery.                                                                                                                                                                                                                                                                                                              |\n|102       | Jessica, subject 32 year old female, third and weak pregnant in active labor, contractions every 3 to 5 minutes, object 2, fetal heart tones, stable, patient is 6 centimeter dilated, 100% effaced, GBS positive, assessment, active labor, plan, admit to L and D, start Penthaline G for GBS, notice Anastasia.                                                                                                                                                                                                                                                                                             |\n|103       | Patient John 101, subject to 58-year-old male history of hyperattention presents with severe crushing chest pain, objective TCG shows ST segment elevation, troponin is marked elevated BNB high assessment, acute myocardial infraction, plan admit to cardiac unit, consult cardiology.                                                                                                                                                                                                                                                                                                                      |\n|104       | David Onzerofel, subject to 67-year-old male, acute right-side weakness, symptoms started on our ego. Object to CT head is negative obliter, MRI confirms left MCA ishemic stroke, NIH score is 11th. Assessment, acute ishemic stroke, plan, administer TBA per protocol, and shorten neuro ICU.                                                                                                                                                                                                                                                                                                              |\n|105       | Emily 108, subject 2, 6 month old female, with respiratory distress and poor feeding, objective, audible phasing and intercostal retractions, nasal swab is born to, for RSV, as a cement bronchialitis, secondary to RSV, plan, add me to pdiatrics, provide cool mist and nasal suctioning.                                                                                                                                                                                                                                                                                                                  |\n|106       | Kevin 109 Subjective 29-year-old male admitted for acute depressive episode with pause-bass you suicidal ideation objective urine, dry, drug screen negative patient is cooperative but has a flat effect assessment major depressive disorder with suicidal ideation plan admit to psychiatrist start one to one observation                                                                                                                                                                                                                                                                                  |\n|107       | The patient is a fortifier year old male presenting with a three-day history of severe chest pain and shortness of breath. He describes the pain as a crushing pressure. He has a history of hypertension and type 2 diabetes. Vital strengths are stable. ECG shows ST segment elevation in the anterior leads, proponent levels are elevated. Patient is diagnosed with an acute myocondyl infection. We will admit the patient to the cardiac unit for continuous monitoring, we will start him on aspirin, lysinopryl and start him, we will also consult cardiology for an urgent cardia characterization.|\n|108       | Note 4 James Smith 102 Subjective 72-year-old female presented after a fall, severe right-hit pain unable to bear weight, objective, right leg shortened and externally rotated, X-ray confirmed femoral neck fracture, HGB is 9.5, assessment, accurate right-hit fracture, plan and pivot for surgery, or the consult for fixation and manage pain with PCA post-op.                                                                                                                                                                                                                                         |\n|109       | Sara, 35 year old, female, acute, epigastric pain, radiating to back with nausea, objective analyze his 950, liposis 1200, assessment, acute pancreas, plan, start morphine, order right upper quadrant, ultrasound.                                                                                                                                                                                                                                                                                                                                                                                           |\n|110       | For our brown 103 subject 68 year old male with a 3 day history of worsening cough fever 101.5 shortness of breath objective o2 saturation 92% on 2 liters chest x ray right lower low infiltrates abg shows respiratory acidosis assessment pneumonia plan admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen.                                                                                                                                                                                                                                                                         |\n|111       | Patient 203, back pain, not for patient 203, subjective, 45 years old male, lifted a heavy box and felt a poop, has sharp lower back pain, radiating down his right leg, objective, positive straight leg rise, touched on the right, numbness is the right foot, assessment, Herinated disc with a sciatic plant prescribed ibophium and a muscle relaxer order on MRI of the lumbar spine.                                                                                                                                                                                                                   |\n|112       | Patient 202, Strip Throat, Notepad Patient 202, Subjective 208's Old Female, Report Search, Soar Throat, Throat and Fever for 2 Days, Objective, Temperature is 102.5, Throat is Red, With Tonsular Exigrated, Exidate, Assessment, Suspect Strip, Fahrenheit, Plan, run a rapid strip test if positive start peninsula.                                                                                                                                                                                                                                                                                       |\n|113       | Patient 201 kidney disease, note for patient 201, subjective 68 years old male with history of diabetic, report new legs feeling and fatigue, objective, bed blood pressure is high at 160 by 90, lab shows that GFR is 25 and keratinine is 2.8 acid, assessment, chronic kidney disease, stage 4 plan refers to nephrology, starts listening to nopriel for bed pleasure.                                                                                                                                                                                                                                    |\n|114       | Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells.                                                                                                                                                                                                                                                                     |\n|115       | Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells.                                                                                                                                                                                                                                                                     |\n|116       | Patient 206, Acetic Reflex, Notework Patient 206, Subjective 38 years old male reports burning chest pain after meals, especially at night, waking up with a sore test objective. If you go strict, area is tender to touch, rest of exam is normal, assessment, GERD, gastro isophagal reflex disease, plan start trial on omifrage parasol daily, advise diet changes like awarding spicy food.                                                                                                                                                                                                              |\n|117       | Patient 208, anxiety, note for patient 208, subjective, 28 years old female, reports constant worry, panic attacks and poor sleep for three months, its interfering with her work, objective, patient is restless and tense, assessment, generalizer, anxiety disorder, and refer to CBT, cognitive behavior therapy, started Lotus SSRI.                                                                                                                                                                                                                                                                      |\n|118       | Patient 209, thyroid, note for patient 209, subject to 42 years old, female, reports, weight gain, despots, no change in diet, plus fatigue and feeling cold, all the time, objective, dry skin, brittle hair, TSH, lab results, 12.5, high, assessment, hypothyroidism, and start patient-on-leave-o tyronexin 50 micrograms daily, reject TSH in 66.                                                                                                                                                                                                                                                         |\n|119       | Patient 207 allergy, Note 4 Patient 207, Subjective, 19 years old male, 8th kid, a party now has lips feeling and difficult breathing. He has a known peanut allergy, objective, audible whizzing, heavies on a chest and neck, assessment, anaphylaxis, plan, administrator, Keep in mind, Phirin, I am immediately. Gyu, Diffinant, Head, Raymond, Transfer to ER.                                                                                                                                                                                                                                           |\n|120       | Patient, Tutan, Ankle injury, not for patient, Tutan, subjective, 16 years old, made, Pisterous, Ankle playing basketball, he had a pop objective, significance, feeling and cruising on lateral, Ankle patient is unable to bear weight, assessment, suspect, Ankle texture, node dust screen, plane, plan, order and X-ray for ankle, for now RSE protocol, reached ice, compressive evaluation.                                                                                                                                                                                                             |\n+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. PIPELINE STEP 1: LOAD & TRANSCRIBE ALL AUDIO (BRONZE)\n",
    "# =========================================================\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import torch\n",
    "\n",
    "print(\"Loading Whisper model for transcription...\")\n",
    "\n",
    "# For Databricks, check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Update path to Databricks Volume\n",
    "audio_folder = \"/Volumes/workspace/default/audio_files/\"\n",
    "audio_files = glob.glob(audio_folder + \"*.wav\")  # You can change .wav to .mp3 if needed\n",
    "\n",
    "transcripts_list = []\n",
    "print(f\"Found {len(audio_files)} audio files. Starting transcription...\")\n",
    "\n",
    "# This is your Python batch loop\n",
    "for audio_file_path in audio_files:\n",
    "    # Get the patient ID from the filename\n",
    "    patient_id = os.path.basename(audio_file_path).split('.')[0]\n",
    "    \n",
    "    # Load the audio\n",
    "    speech, sample_rate = librosa.load(audio_file_path, sr=16000)\n",
    "    \n",
    "    # Transcribe\n",
    "    result = asr_pipeline(speech, return_timestamps=True)\n",
    "    transcript_text = result['text']\n",
    "    \n",
    "    # Add the result to our list\n",
    "    transcripts_list.append((patient_id, transcript_text))\n",
    "    print(f\"  > Transcribed Patient ID: {patient_id}\")\n",
    "\n",
    "# --- Create the first PySpark DataFrame ---\n",
    "transcript_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"transcript_text\", StringType(), True)\n",
    "])\n",
    "\n",
    "transcripts_df = spark.createDataFrame(transcripts_list, schema=transcript_schema)\n",
    "\n",
    "print(\"\\n--- Audio Transcripts DataFrame ---\")\n",
    "transcripts_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c947854-e311-43be-94d7-d06506448304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>transcript_text</th></tr></thead><tbody><tr><td>101</td><td> Michael 107, subject to post-operation day 3, appendectomy, reports worsening incision pain and fever, objective fever of 102, white count 14.2, incision site is red, warm and draining fluid, assessment, surgical site infection, plan send wood culture, start bankomacinal, consult surgery.</td></tr><tr><td>102</td><td> Jessica, subject 32 year old female, third and weak pregnant in active labor, contractions every 3 to 5 minutes, object 2, fetal heart tones, stable, patient is 6 centimeter dilated, 100% effaced, GBS positive, assessment, active labor, plan, admit to L and D, start Penthaline G for GBS, notice Anastasia.</td></tr><tr><td>103</td><td> Patient John 101, subject to 58-year-old male history of hyperattention presents with severe crushing chest pain, objective TCG shows ST segment elevation, troponin is marked elevated BNB high assessment, acute myocardial infraction, plan admit to cardiac unit, consult cardiology.</td></tr><tr><td>104</td><td> David Onzerofel, subject to 67-year-old male, acute right-side weakness, symptoms started on our ego. Object to CT head is negative obliter, MRI confirms left MCA ishemic stroke, NIH score is 11th. Assessment, acute ishemic stroke, plan, administer TBA per protocol, and shorten neuro ICU.</td></tr><tr><td>105</td><td> Emily 108, subject 2, 6 month old female, with respiratory distress and poor feeding, objective, audible phasing and intercostal retractions, nasal swab is born to, for RSV, as a cement bronchialitis, secondary to RSV, plan, add me to pdiatrics, provide cool mist and nasal suctioning.</td></tr><tr><td>106</td><td> Kevin 109 Subjective 29-year-old male admitted for acute depressive episode with pause-bass you suicidal ideation objective urine, dry, drug screen negative patient is cooperative but has a flat effect assessment major depressive disorder with suicidal ideation plan admit to psychiatrist start one to one observation</td></tr><tr><td>107</td><td> The patient is a fortifier year old male presenting with a three-day history of severe chest pain and shortness of breath. He describes the pain as a crushing pressure. He has a history of hypertension and type 2 diabetes. Vital strengths are stable. ECG shows ST segment elevation in the anterior leads, proponent levels are elevated. Patient is diagnosed with an acute myocondyl infection. We will admit the patient to the cardiac unit for continuous monitoring, we will start him on aspirin, lysinopryl and start him, we will also consult cardiology for an urgent cardia characterization.</td></tr><tr><td>108</td><td> Note 4 James Smith 102 Subjective 72-year-old female presented after a fall, severe right-hit pain unable to bear weight, objective, right leg shortened and externally rotated, X-ray confirmed femoral neck fracture, HGB is 9.5, assessment, accurate right-hit fracture, plan and pivot for surgery, or the consult for fixation and manage pain with PCA post-op.</td></tr><tr><td>109</td><td> Sara, 35 year old, female, acute, epigastric pain, radiating to back with nausea, objective analyze his 950, liposis 1200, assessment, acute pancreas, plan, start morphine, order right upper quadrant, ultrasound.</td></tr><tr><td>110</td><td> For our brown 103 subject 68 year old male with a 3 day history of worsening cough fever 101.5 shortness of breath objective o2 saturation 92% on 2 liters chest x ray right lower low infiltrates abg shows respiratory acidosis assessment pneumonia plan admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen.</td></tr><tr><td>111</td><td> Patient 203, back pain, not for patient 203, subjective, 45 years old male, lifted a heavy box and felt a poop, has sharp lower back pain, radiating down his right leg, objective, positive straight leg rise, touched on the right, numbness is the right foot, assessment, Herinated disc with a sciatic plant prescribed ibophium and a muscle relaxer order on MRI of the lumbar spine.</td></tr><tr><td>112</td><td> Patient 202, Strip Throat, Notepad Patient 202, Subjective 208's Old Female, Report Search, Soar Throat, Throat and Fever for 2 Days, Objective, Temperature is 102.5, Throat is Red, With Tonsular Exigrated, Exidate, Assessment, Suspect Strip, Fahrenheit, Plan, run a rapid strip test if positive start peninsula.</td></tr><tr><td>113</td><td> Patient 201 kidney disease, note for patient 201, subjective 68 years old male with history of diabetic, report new legs feeling and fatigue, objective, bed blood pressure is high at 160 by 90, lab shows that GFR is 25 and keratinine is 2.8 acid, assessment, chronic kidney disease, stage 4 plan refers to nephrology, starts listening to nopriel for bed pleasure.</td></tr><tr><td>114</td><td> Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells.</td></tr><tr><td>115</td><td> Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells.</td></tr><tr><td>116</td><td> Patient 206, Acetic Reflex, Notework Patient 206, Subjective 38 years old male reports burning chest pain after meals, especially at night, waking up with a sore test objective. If you go strict, area is tender to touch, rest of exam is normal, assessment, GERD, gastro isophagal reflex disease, plan start trial on omifrage parasol daily, advise diet changes like awarding spicy food.</td></tr><tr><td>117</td><td> Patient 208, anxiety, note for patient 208, subjective, 28 years old female, reports constant worry, panic attacks and poor sleep for three months, its interfering with her work, objective, patient is restless and tense, assessment, generalizer, anxiety disorder, and refer to CBT, cognitive behavior therapy, started Lotus SSRI.</td></tr><tr><td>118</td><td> Patient 209, thyroid, note for patient 209, subject to 42 years old, female, reports, weight gain, despots, no change in diet, plus fatigue and feeling cold, all the time, objective, dry skin, brittle hair, TSH, lab results, 12.5, high, assessment, hypothyroidism, and start patient-on-leave-o tyronexin 50 micrograms daily, reject TSH in 66.</td></tr><tr><td>119</td><td> Patient 207 allergy, Note 4 Patient 207, Subjective, 19 years old male, 8th kid, a party now has lips feeling and difficult breathing. He has a known peanut allergy, objective, audible whizzing, heavies on a chest and neck, assessment, anaphylaxis, plan, administrator, Keep in mind, Phirin, I am immediately. Gyu, Diffinant, Head, Raymond, Transfer to ER.</td></tr><tr><td>120</td><td> Patient, Tutan, Ankle injury, not for patient, Tutan, subjective, 16 years old, made, Pisterous, Ankle playing basketball, he had a pop objective, significance, feeling and cruising on lateral, Ankle patient is unable to bear weight, assessment, suspect, Ankle texture, node dust screen, plane, plan, order and X-ray for ankle, for now RSE protocol, reached ice, compressive evaluation.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         " Michael 107, subject to post-operation day 3, appendectomy, reports worsening incision pain and fever, objective fever of 102, white count 14.2, incision site is red, warm and draining fluid, assessment, surgical site infection, plan send wood culture, start bankomacinal, consult surgery."
        ],
        [
         "102",
         " Jessica, subject 32 year old female, third and weak pregnant in active labor, contractions every 3 to 5 minutes, object 2, fetal heart tones, stable, patient is 6 centimeter dilated, 100% effaced, GBS positive, assessment, active labor, plan, admit to L and D, start Penthaline G for GBS, notice Anastasia."
        ],
        [
         "103",
         " Patient John 101, subject to 58-year-old male history of hyperattention presents with severe crushing chest pain, objective TCG shows ST segment elevation, troponin is marked elevated BNB high assessment, acute myocardial infraction, plan admit to cardiac unit, consult cardiology."
        ],
        [
         "104",
         " David Onzerofel, subject to 67-year-old male, acute right-side weakness, symptoms started on our ego. Object to CT head is negative obliter, MRI confirms left MCA ishemic stroke, NIH score is 11th. Assessment, acute ishemic stroke, plan, administer TBA per protocol, and shorten neuro ICU."
        ],
        [
         "105",
         " Emily 108, subject 2, 6 month old female, with respiratory distress and poor feeding, objective, audible phasing and intercostal retractions, nasal swab is born to, for RSV, as a cement bronchialitis, secondary to RSV, plan, add me to pdiatrics, provide cool mist and nasal suctioning."
        ],
        [
         "106",
         " Kevin 109 Subjective 29-year-old male admitted for acute depressive episode with pause-bass you suicidal ideation objective urine, dry, drug screen negative patient is cooperative but has a flat effect assessment major depressive disorder with suicidal ideation plan admit to psychiatrist start one to one observation"
        ],
        [
         "107",
         " The patient is a fortifier year old male presenting with a three-day history of severe chest pain and shortness of breath. He describes the pain as a crushing pressure. He has a history of hypertension and type 2 diabetes. Vital strengths are stable. ECG shows ST segment elevation in the anterior leads, proponent levels are elevated. Patient is diagnosed with an acute myocondyl infection. We will admit the patient to the cardiac unit for continuous monitoring, we will start him on aspirin, lysinopryl and start him, we will also consult cardiology for an urgent cardia characterization."
        ],
        [
         "108",
         " Note 4 James Smith 102 Subjective 72-year-old female presented after a fall, severe right-hit pain unable to bear weight, objective, right leg shortened and externally rotated, X-ray confirmed femoral neck fracture, HGB is 9.5, assessment, accurate right-hit fracture, plan and pivot for surgery, or the consult for fixation and manage pain with PCA post-op."
        ],
        [
         "109",
         " Sara, 35 year old, female, acute, epigastric pain, radiating to back with nausea, objective analyze his 950, liposis 1200, assessment, acute pancreas, plan, start morphine, order right upper quadrant, ultrasound."
        ],
        [
         "110",
         " For our brown 103 subject 68 year old male with a 3 day history of worsening cough fever 101.5 shortness of breath objective o2 saturation 92% on 2 liters chest x ray right lower low infiltrates abg shows respiratory acidosis assessment pneumonia plan admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen."
        ],
        [
         "111",
         " Patient 203, back pain, not for patient 203, subjective, 45 years old male, lifted a heavy box and felt a poop, has sharp lower back pain, radiating down his right leg, objective, positive straight leg rise, touched on the right, numbness is the right foot, assessment, Herinated disc with a sciatic plant prescribed ibophium and a muscle relaxer order on MRI of the lumbar spine."
        ],
        [
         "112",
         " Patient 202, Strip Throat, Notepad Patient 202, Subjective 208's Old Female, Report Search, Soar Throat, Throat and Fever for 2 Days, Objective, Temperature is 102.5, Throat is Red, With Tonsular Exigrated, Exidate, Assessment, Suspect Strip, Fahrenheit, Plan, run a rapid strip test if positive start peninsula."
        ],
        [
         "113",
         " Patient 201 kidney disease, note for patient 201, subjective 68 years old male with history of diabetic, report new legs feeling and fatigue, objective, bed blood pressure is high at 160 by 90, lab shows that GFR is 25 and keratinine is 2.8 acid, assessment, chronic kidney disease, stage 4 plan refers to nephrology, starts listening to nopriel for bed pleasure."
        ],
        [
         "114",
         " Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells."
        ],
        [
         "115",
         " Patient 205. Anemia. Note for Patient 205. Subjective. Seventy-years-old female reports extreme fatigue, dezenous and feeling short of breath when walking. Objective. Patient is very pale. Lab shows hemoglobin of 7.2. Assessment. Several Anemia. Plan. Admit hospital type and cross for two units of packed packed red blade shells."
        ],
        [
         "116",
         " Patient 206, Acetic Reflex, Notework Patient 206, Subjective 38 years old male reports burning chest pain after meals, especially at night, waking up with a sore test objective. If you go strict, area is tender to touch, rest of exam is normal, assessment, GERD, gastro isophagal reflex disease, plan start trial on omifrage parasol daily, advise diet changes like awarding spicy food."
        ],
        [
         "117",
         " Patient 208, anxiety, note for patient 208, subjective, 28 years old female, reports constant worry, panic attacks and poor sleep for three months, its interfering with her work, objective, patient is restless and tense, assessment, generalizer, anxiety disorder, and refer to CBT, cognitive behavior therapy, started Lotus SSRI."
        ],
        [
         "118",
         " Patient 209, thyroid, note for patient 209, subject to 42 years old, female, reports, weight gain, despots, no change in diet, plus fatigue and feeling cold, all the time, objective, dry skin, brittle hair, TSH, lab results, 12.5, high, assessment, hypothyroidism, and start patient-on-leave-o tyronexin 50 micrograms daily, reject TSH in 66."
        ],
        [
         "119",
         " Patient 207 allergy, Note 4 Patient 207, Subjective, 19 years old male, 8th kid, a party now has lips feeling and difficult breathing. He has a known peanut allergy, objective, audible whizzing, heavies on a chest and neck, assessment, anaphylaxis, plan, administrator, Keep in mind, Phirin, I am immediately. Gyu, Diffinant, Head, Raymond, Transfer to ER."
        ],
        [
         "120",
         " Patient, Tutan, Ankle injury, not for patient, Tutan, subjective, 16 years old, made, Pisterous, Ankle playing basketball, he had a pop objective, significance, feeling and cruising on lateral, Ankle patient is unable to bear weight, assessment, suspect, Ankle texture, node dust screen, plane, plan, order and X-ray for ankle, for now RSE protocol, reached ice, compressive evaluation."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transcript_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(transcripts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbab562-641f-4956-83f8-df2d9a4e0078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Initializing Question Answering (QA) AI Model (on CPU) ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting notes to run AI extraction...\n > Processed Patient ID: 101\n > Processed Patient ID: 102\n > Processed Patient ID: 103\n > Processed Patient ID: 104\n > Processed Patient ID: 105\n > Processed Patient ID: 106\n > Processed Patient ID: 107\n > Processed Patient ID: 108\n > Processed Patient ID: 109\n > Processed Patient ID: 110\n > Processed Patient ID: 201\n > Processed Patient ID: 202\n > Processed Patient ID: 203\n > Processed Patient ID: 205\n > Processed Patient ID: 206\n > Processed Patient ID: 207\n > Processed Patient ID: 208\n > Processed Patient ID: 209\n > Processed Patient ID: 210\n\n--- AI-Extracted Nursing Notes Silver Table ---\n+---------------------------------------+----------+---------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------+\n|assessment                             |patient_id|patient_problem                              |plan                                                               |vitals                                             |\n+---------------------------------------+----------+---------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------+\n|8/10 chest pain                        |101       |chest pain                                   |Family anxious                                                     |chest pain                                         |\n|Vitals stable                          |102       |pain                                         |NULL                                                               |pain                                               |\n|92%                                    |103       |cough and fever                              |Started on IV antibiotics                                          |cough and fever                                    |\n|Diabetes educator consulted            |104       |DKA                                          |Started on insulin drip per protocol                               |Diabetes                                           |\n|NIH Stroke Scale: 12                   |105       |aphasia                                      |NIH Stroke Scale: 12                                               |weakness and aphasia                               |\n|Receiving IV fluids and pain management|106       |acute-onset abdominal pain                   |Receiving IV fluids and pain management                            |acute-onset abdominal pain, nausea, and vomiting   |\n|NULL                                   |107       |infection                                    |NULL                                                               |purulent fluid                                     |\n|Suctioned for secretions PRN           |108       |respiratory distress                         |Suctioned for secretions PRN                                       |secretions                                         |\n|NULL                                   |109       |suicidal ideation                            |Safety plan                                                        |acute depressive episode with suicidal ideation    |\n|Epidural placed                        |110       |Patient is 39 weeks, admitted in active labor|Receiving Penicillin G for GBS prophylaxis                         |NULL                                               |\n|Stage 4 Chronic Kidney Disease         |201       |fatigue                                      |NULL                                                               |leg swelling and fatigue                           |\n|Rapid strep test                       |202       |sore throat and fever                        |Will start penicillin                                              |sore throat and fever                              |\n|Suspected herniated disc with sciatica |203       |lower back pain                              |lumbar spine MRI                                                   |lumbar spine MRI                                   |\n|Severe Anemia                          |205       |dizziness                                    |Admitting to hospital for 2 units packed red blood cell transfusion|extreme fatigue, dizziness, and shortness of breath|\n|GERD                                   |206       |burning chest pain                           |Started on daily omeprazole trial                                  |chest pain                                         |\n|Transferring to ER                     |207       |difficulty breathing                         |Transferring to ER for further monitoring                          |acute lip swelling                                 |\n|GAD                                    |208       |panic attacks                                |States symptoms are interfering with her work                      |constant worry, panic attacks, and poor sleep      |\n|weight gain                            |209       |weight gain                                  |Follow-up TSH lab scheduled for 6 weeks                            |weight gain                                        |\n|NULL                                   |210       |Suspect fracture                             |RICE protocol initiated                                            |swelling and bruising                              |\n+---------------------------------------+----------+---------------------------------------------+-------------------------------------------------------------------+---------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"patient_notes\")\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "from transformers import pipeline\n",
    "\n",
    "# =========================================================\n",
    "# AI PREPROCESSING: EXTRACT FROM NURSING NOTES (SILVER)\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n--- Initializing Question Answering (QA) AI Model (on CPU) ---\")\n",
    "# Load a model fine-tuned for question answering\n",
    "# device=-1 explicitly tells it to use the CPU\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=\"distilbert-base-cased-distilled-squad\", \n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "\n",
    "# --- Step 1: Combine notes from your *existing* 'df' DataFrame ---\n",
    "# This line is changed to use 'df' as the source\n",
    "nursing_notes_text_df = df.select(\n",
    "    col(\"patient_id\"),\n",
    "    concat_ws(\" \", col(\"nursing_notes\")).alias(\"notes_context\") # Join all notes with a space\n",
    ")\n",
    "\n",
    "# --- Step 2: Collect the notes to the driver node ---\n",
    "print(\"Collecting notes to run AI extraction...\")\n",
    "patients_to_process = nursing_notes_text_df.collect()\n",
    "\n",
    "# --- Step 3: Define your questions (your desired columns) ---\n",
    "questions = {\n",
    "    \"patient_problem\": \"What is the patient's primary problem or complaint?\",\n",
    "    \"vitals\": \"What are the patient's vitals?\",\n",
    "    \"assessment\": \"What is the assessment?\",\n",
    "    \"plan\": \"What is the plan?\"\n",
    "}\n",
    "\n",
    "# --- Step 4: Loop, ask questions, and get AI-extracted answers ---\n",
    "extracted_data = []\n",
    "\n",
    "for row in patients_to_process:\n",
    "    patient_id = row[\"patient_id\"]\n",
    "    context = row[\"notes_context\"]\n",
    "    \n",
    "    result = {\"patient_id\": patient_id}\n",
    "    \n",
    "    # Ask each question to the AI model\n",
    "    for column_name, question_text in questions.items():\n",
    "        qa_result = qa_pipeline(question=question_text, context=context)\n",
    "        \n",
    "        # We save the extracted answer.\n",
    "        if qa_result['score'] > 0.1: # You can tune this confidence threshold\n",
    "            result[column_name] = qa_result['answer']\n",
    "        else:\n",
    "            result[column_name] = None # No good answer found\n",
    "            \n",
    "    extracted_data.append(result)\n",
    "    print(f\" > Processed Patient ID: {patient_id}\")\n",
    "\n",
    "# --- Step 5: Create the final Silver DataFrame ---\n",
    "# Convert your list of Python dictionaries into a new Spark DataFrame\n",
    "ai_nursing_silver_df = spark.createDataFrame(extracted_data)\n",
    "\n",
    "print(\"\\n--- AI-Extracted Nursing Notes Silver Table ---\")\n",
    "ai_nursing_silver_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c659193-bef7-4a54-b619-22f44b6d89a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>assessment</th><th>patient_id</th><th>patient_problem</th><th>plan</th><th>vitals</th></tr></thead><tbody><tr><td>8/10 chest pain</td><td>101</td><td>chest pain</td><td>Family anxious</td><td>null</td></tr><tr><td>Vitals stable</td><td>102</td><td>pain</td><td>null</td><td>null</td></tr><tr><td>92%</td><td>103</td><td>cough and fever</td><td>Started on IV antibiotics</td><td>null</td></tr><tr><td>Diabetes educator consulted</td><td>104</td><td>DKA</td><td>Started on insulin drip per protocol</td><td>null</td></tr><tr><td>NIH Stroke Scale: 12</td><td>105</td><td>aphasia</td><td>NIH Stroke Scale: 12</td><td>null</td></tr><tr><td>Receiving IV fluids and pain management</td><td>106</td><td>acute-onset abdominal pain</td><td>Receiving IV fluids and pain management</td><td>null</td></tr><tr><td>null</td><td>107</td><td>infection</td><td>null</td><td>null</td></tr><tr><td>Suctioned for secretions PRN</td><td>108</td><td>respiratory distress</td><td>Suctioned for secretions PRN</td><td>null</td></tr><tr><td>null</td><td>109</td><td>suicidal ideation</td><td>Safety plan</td><td>null</td></tr><tr><td>Epidural placed</td><td>110</td><td>Patient is 39 weeks, admitted in active labor</td><td>Receiving Penicillin G for GBS prophylaxis</td><td>null</td></tr><tr><td>Stage 4 Chronic Kidney Disease</td><td>201</td><td>fatigue</td><td>null</td><td>BP 160/90</td></tr><tr><td>Rapid strep test</td><td>202</td><td>sore throat and fever</td><td>Will start penicillin</td><td>Temp 102</td></tr><tr><td>Suspected herniated disc with sciatica</td><td>203</td><td>lower back pain</td><td>lumbar spine MRI</td><td>null</td></tr><tr><td>Severe Anemia</td><td>205</td><td>dizziness</td><td>Admitting to hospital for 2 units packed red blood cell transfusion</td><td>null</td></tr><tr><td>GERD</td><td>206</td><td>burning chest pain</td><td>Started on daily omeprazole trial</td><td>null</td></tr><tr><td>Transferring to ER</td><td>207</td><td>difficulty breathing</td><td>Transferring to ER for further monitoring</td><td>null</td></tr><tr><td>GAD</td><td>208</td><td>panic attacks</td><td>States symptoms are interfering with her work</td><td>null</td></tr><tr><td>weight gain</td><td>209</td><td>weight gain</td><td>Follow-up TSH lab scheduled for 6 weeks</td><td>null</td></tr><tr><td>null</td><td>210</td><td>Suspect fracture</td><td>RICE protocol initiated</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "8/10 chest pain",
         101,
         "chest pain",
         "Family anxious",
         null
        ],
        [
         "Vitals stable",
         102,
         "pain",
         null,
         null
        ],
        [
         "92%",
         103,
         "cough and fever",
         "Started on IV antibiotics",
         null
        ],
        [
         "Diabetes educator consulted",
         104,
         "DKA",
         "Started on insulin drip per protocol",
         null
        ],
        [
         "NIH Stroke Scale: 12",
         105,
         "aphasia",
         "NIH Stroke Scale: 12",
         null
        ],
        [
         "Receiving IV fluids and pain management",
         106,
         "acute-onset abdominal pain",
         "Receiving IV fluids and pain management",
         null
        ],
        [
         null,
         107,
         "infection",
         null,
         null
        ],
        [
         "Suctioned for secretions PRN",
         108,
         "respiratory distress",
         "Suctioned for secretions PRN",
         null
        ],
        [
         null,
         109,
         "suicidal ideation",
         "Safety plan",
         null
        ],
        [
         "Epidural placed",
         110,
         "Patient is 39 weeks, admitted in active labor",
         "Receiving Penicillin G for GBS prophylaxis",
         null
        ],
        [
         "Stage 4 Chronic Kidney Disease",
         201,
         "fatigue",
         null,
         "BP 160/90"
        ],
        [
         "Rapid strep test",
         202,
         "sore throat and fever",
         "Will start penicillin",
         "Temp 102"
        ],
        [
         "Suspected herniated disc with sciatica",
         203,
         "lower back pain",
         "lumbar spine MRI",
         null
        ],
        [
         "Severe Anemia",
         205,
         "dizziness",
         "Admitting to hospital for 2 units packed red blood cell transfusion",
         null
        ],
        [
         "GERD",
         206,
         "burning chest pain",
         "Started on daily omeprazole trial",
         null
        ],
        [
         "Transferring to ER",
         207,
         "difficulty breathing",
         "Transferring to ER for further monitoring",
         null
        ],
        [
         "GAD",
         208,
         "panic attacks",
         "States symptoms are interfering with her work",
         null
        ],
        [
         "weight gain",
         209,
         "weight gain",
         "Follow-up TSH lab scheduled for 6 weeks",
         null
        ],
        [
         null,
         210,
         "Suspect fracture",
         "RICE protocol initiated",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "assessment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_problem",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "plan",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "vitals",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ai_nursing_silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a34c1fa2-0afe-4e21-a2b0-dc19dbbbd2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Initializing Question Answering (QA) AI Model (on CPU) ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting notes to run AI extraction...\n > Processed Patient ID: 101\n > Processed Patient ID: 102\n > Processed Patient ID: 103\n > Processed Patient ID: 104\n > Processed Patient ID: 105\n > Processed Patient ID: 106\n > Processed Patient ID: 107\n > Processed Patient ID: 108\n > Processed Patient ID: 109\n > Processed Patient ID: 110\n > Processed Patient ID: 201\n > Processed Patient ID: 202\n > Processed Patient ID: 203\n > Processed Patient ID: 205\n > Processed Patient ID: 206\n > Processed Patient ID: 207\n > Processed Patient ID: 208\n > Processed Patient ID: 209\n > Processed Patient ID: 210\n\n--- AI-Extracted Nursing Notes Silver Table (Hybrid + Other) ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_problem</th><th>vitals</th><th>assessment</th><th>plan</th></tr></thead><tbody><tr><td>101</td><td>chest pain</td><td>null</td><td>8/10 chest pain</td><td>Family anxious</td></tr><tr><td>102</td><td>pain</td><td>null</td><td>Vitals stable</td><td>null</td></tr><tr><td>103</td><td>cough and fever</td><td>null</td><td>92%</td><td>Started on IV antibiotics</td></tr><tr><td>104</td><td>DKA</td><td>null</td><td>Diabetes educator consulted</td><td>Started on insulin drip per protocol</td></tr><tr><td>105</td><td>aphasia</td><td>null</td><td>NIH Stroke Scale: 12</td><td>NIH Stroke Scale: 12</td></tr><tr><td>106</td><td>acute-onset abdominal pain</td><td>null</td><td>Receiving IV fluids and pain management</td><td>Receiving IV fluids and pain management</td></tr><tr><td>107</td><td>infection</td><td>null</td><td>null</td><td>null</td></tr><tr><td>108</td><td>respiratory distress</td><td>null</td><td>Suctioned for secretions PRN</td><td>Suctioned for secretions PRN</td></tr><tr><td>109</td><td>suicidal ideation</td><td>null</td><td>null</td><td>Safety plan</td></tr><tr><td>110</td><td>Patient is 39 weeks, admitted in active labor</td><td>null</td><td>Epidural placed</td><td>Receiving Penicillin G for GBS prophylaxis</td></tr><tr><td>201</td><td>fatigue</td><td>BP 160/90</td><td>Stage 4 Chronic Kidney Disease</td><td>null</td></tr><tr><td>202</td><td>sore throat and fever</td><td>Temp 102</td><td>Rapid strep test</td><td>Will start penicillin</td></tr><tr><td>203</td><td>lower back pain</td><td>null</td><td>Suspected herniated disc with sciatica</td><td>lumbar spine MRI</td></tr><tr><td>205</td><td>dizziness</td><td>null</td><td>Severe Anemia</td><td>Admitting to hospital for 2 units packed red blood cell transfusion</td></tr><tr><td>206</td><td>burning chest pain</td><td>null</td><td>GERD</td><td>Started on daily omeprazole trial</td></tr><tr><td>207</td><td>difficulty breathing</td><td>null</td><td>Transferring to ER</td><td>Transferring to ER for further monitoring</td></tr><tr><td>208</td><td>panic attacks</td><td>null</td><td>GAD</td><td>States symptoms are interfering with her work</td></tr><tr><td>209</td><td>weight gain</td><td>null</td><td>weight gain</td><td>Follow-up TSH lab scheduled for 6 weeks</td></tr><tr><td>210</td><td>Suspect fracture</td><td>null</td><td>null</td><td>RICE protocol initiated</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "chest pain",
         null,
         "8/10 chest pain",
         "Family anxious"
        ],
        [
         102,
         "pain",
         null,
         "Vitals stable",
         null
        ],
        [
         103,
         "cough and fever",
         null,
         "92%",
         "Started on IV antibiotics"
        ],
        [
         104,
         "DKA",
         null,
         "Diabetes educator consulted",
         "Started on insulin drip per protocol"
        ],
        [
         105,
         "aphasia",
         null,
         "NIH Stroke Scale: 12",
         "NIH Stroke Scale: 12"
        ],
        [
         106,
         "acute-onset abdominal pain",
         null,
         "Receiving IV fluids and pain management",
         "Receiving IV fluids and pain management"
        ],
        [
         107,
         "infection",
         null,
         null,
         null
        ],
        [
         108,
         "respiratory distress",
         null,
         "Suctioned for secretions PRN",
         "Suctioned for secretions PRN"
        ],
        [
         109,
         "suicidal ideation",
         null,
         null,
         "Safety plan"
        ],
        [
         110,
         "Patient is 39 weeks, admitted in active labor",
         null,
         "Epidural placed",
         "Receiving Penicillin G for GBS prophylaxis"
        ],
        [
         201,
         "fatigue",
         "BP 160/90",
         "Stage 4 Chronic Kidney Disease",
         null
        ],
        [
         202,
         "sore throat and fever",
         "Temp 102",
         "Rapid strep test",
         "Will start penicillin"
        ],
        [
         203,
         "lower back pain",
         null,
         "Suspected herniated disc with sciatica",
         "lumbar spine MRI"
        ],
        [
         205,
         "dizziness",
         null,
         "Severe Anemia",
         "Admitting to hospital for 2 units packed red blood cell transfusion"
        ],
        [
         206,
         "burning chest pain",
         null,
         "GERD",
         "Started on daily omeprazole trial"
        ],
        [
         207,
         "difficulty breathing",
         null,
         "Transferring to ER",
         "Transferring to ER for further monitoring"
        ],
        [
         208,
         "panic attacks",
         null,
         "GAD",
         "States symptoms are interfering with her work"
        ],
        [
         209,
         "weight gain",
         null,
         "weight gain",
         "Follow-up TSH lab scheduled for 6 weeks"
        ],
        [
         210,
         "Suspect fracture",
         null,
         null,
         "RICE protocol initiated"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_problem",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "vitals",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "assessment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "plan",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re  # Import regular expressions for cleaning\n",
    "from pyspark.sql.functions import col, concat_ws, regexp_extract\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load your DataFrame from the table\n",
    "df = spark.read.table(\"patient_notes\")\n",
    "\n",
    "# =========================================================\n",
    "# AI PREPROCESSING: EXTRACT FROM NURSING NOTES (SILVER)\n",
    "# HYBRID APPROACH (REGEX + AI + CATCH-ALL)\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n--- Initializing Question Answering (QA) AI Model (on CPU) ---\")\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\",\n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "\n",
    "# --- Step 1: Pre-process the notes from 'df' ---\n",
    "nursing_notes_hybrid_df = df.select(\n",
    "    col(\"patient_id\"),\n",
    "    # Create the single text blob for the AI\n",
    "    concat_ws(\" \", col(\"nursing_notes\")).alias(\"notes_context\"),\n",
    "\n",
    "    # --- USE REGEX FOR VITALS ---\n",
    "    # We'll extract any text after \"Vitals: \" and before the next period \".\"\n",
    "    regexp_extract(\n",
    "        concat_ws(\" \", col(\"nursing_notes\")),\n",
    "        r\"Vitals: (.*?)\\.\",\n",
    "        1\n",
    "    ).alias(\"vitals_extract\") # Renamed to avoid confusion\n",
    ")\n",
    "\n",
    "# --- Step 2: Collect the data to the driver node ---\n",
    "print(\"Collecting notes to run AI extraction...\")\n",
    "patients_to_process = nursing_notes_hybrid_df.collect()\n",
    "\n",
    "# --- Step 3: Define *only* the hard questions for the AI ---\n",
    "questions = {\n",
    "    \"patient_problem\": \"What is the patient's primary problem or complaint?\",\n",
    "    \"assessment\": \"What is the assessment?\",\n",
    "    \"plan\": \"What is the plan?\"\n",
    "}\n",
    "\n",
    "# --- Step 4: Loop, ask questions, and get AI-extracted answers ---\n",
    "extracted_data = []\n",
    "\n",
    "for row in patients_to_process:\n",
    "    patient_id = row[\"patient_id\"]\n",
    "    context = row[\"notes_context\"]\n",
    "    \n",
    "    result = {\"patient_id\": patient_id}\n",
    "    \n",
    "    # This list will hold all the pieces of text we extract\n",
    "    extracted_pieces = []\n",
    "    \n",
    "    # --- 1. Get Vitals (Regex) ---\n",
    "    vitals_text = row[\"vitals_extract\"] if row[\"vitals_extract\"] else None\n",
    "    if vitals_text:\n",
    "        result[\"vitals\"] = vitals_text\n",
    "        # Add the extracted text AND the keyword to the removal list\n",
    "        extracted_pieces.append(vitals_text)\n",
    "        extracted_pieces.append(\"Vitals:\")\n",
    "    else:\n",
    "        result[\"vitals\"] = None\n",
    "\n",
    "    # --- 2. Get AI Answers ---\n",
    "    for column_name, question_text in questions.items():\n",
    "        qa_result = qa_pipeline(question=question_text, context=context)\n",
    "        \n",
    "        if qa_result['score'] > 0.1: # You can tune this confidence threshold\n",
    "            answer = qa_result['answer']\n",
    "            result[column_name] = answer\n",
    "            # Add the AI's answer to the removal list\n",
    "            extracted_pieces.append(answer)\n",
    "        else:\n",
    "            result[column_name] = None\n",
    "\n",
    "    \n",
    "    extracted_data.append(result)\n",
    "    print(f\" > Processed Patient ID: {patient_id}\")\n",
    "\n",
    "# --- Step 5: Create the final Silver DataFrame ---\n",
    "ai_nursing_silver_df = spark.createDataFrame(extracted_data)\n",
    "\n",
    "# Reorder columns to be more logical\n",
    "column_order = [\n",
    "    \"patient_id\",\n",
    "    \"patient_problem\",\n",
    "    \"vitals\",\n",
    "    \"assessment\",\n",
    "    \"plan\"\n",
    "]\n",
    "ai_nursing_silver_df = ai_nursing_silver_df.select(column_order)\n",
    "\n",
    "\n",
    "print(\"\\n--- AI-Extracted Nursing Notes Silver Table (Hybrid + Other) ---\")\n",
    "display(ai_nursing_silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e10f71-7147-4777-b921-f35982542a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Creating Silver Table: Lab Results ---\n\n--- Lab Results Silver Table ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_name</th><th>test</th><th>result</th><th>date</th></tr></thead><tbody><tr><td>101</td><td>John Doe</td><td>Troponin</td><td>Elevated</td><td>2025-11-10</td></tr><tr><td>101</td><td>John Doe</td><td>ECG</td><td>ST-segment elevation</td><td>2025-11-10</td></tr><tr><td>101</td><td>John Doe</td><td>BNP</td><td>High</td><td>2025-11-11</td></tr><tr><td>102</td><td>Jane Smith</td><td>X-Ray (Right Hip)</td><td>Femoral neck fracture</td><td>2025-11-09</td></tr><tr><td>102</td><td>Jane Smith</td><td>Hgb</td><td>9.5 g/dL (Low)</td><td>2025-11-10</td></tr><tr><td>103</td><td>Robert Brown</td><td>Sputum Culture</td><td>Pending</td><td>2025-11-11</td></tr><tr><td>103</td><td>Robert Brown</td><td>ABG</td><td>Respiratory acidosis</td><td>2025-11-11</td></tr><tr><td>103</td><td>Robert Brown</td><td>Chest X-Ray</td><td>Infiltrates in right lower lobe</td><td>2025-11-10</td></tr><tr><td>104</td><td>Maria Garcia</td><td>Blood Glucose</td><td>450 mg/dL (High)</td><td>2025-11-11</td></tr><tr><td>104</td><td>Maria Garcia</td><td>Urinalysis</td><td>Ketones present</td><td>2025-11-11</td></tr><tr><td>104</td><td>Maria Garcia</td><td>A1c</td><td>11.2%</td><td>2025-11-11</td></tr><tr><td>105</td><td>David Lee</td><td>CT Head (Non-contrast)</td><td>No acute bleed</td><td>2025-11-08</td></tr><tr><td>105</td><td>David Lee</td><td>MRI Brain</td><td>Acute ischemic stroke, left MCA</td><td>2025-11-09</td></tr><tr><td>106</td><td>Sarah Chen</td><td>Amylase</td><td>950 U/L (High)</td><td>2025-11-10</td></tr><tr><td>106</td><td>Sarah Chen</td><td>Lipase</td><td>1200 U/L (High)</td><td>2025-11-10</td></tr><tr><td>107</td><td>Michael Johnson</td><td>WBC</td><td>14.2 k/uL (High)</td><td>2025-11-11</td></tr><tr><td>107</td><td>Michael Johnson</td><td>Wound Culture</td><td>Pending</td><td>2025-11-11</td></tr><tr><td>108</td><td>Emily White</td><td>RSV Swab</td><td>Positive</td><td>2025-11-10</td></tr><tr><td>109</td><td>Kevin Patel</td><td>Urine Drug Screen</td><td>Negative</td><td>2025-11-09</td></tr><tr><td>109</td><td>Kevin Patel</td><td>TSH</td><td>Within normal limits</td><td>2025-11-09</td></tr><tr><td>110</td><td>Jessica Davis</td><td>GBS</td><td>Positive</td><td>2025-11-01</td></tr><tr><td>110</td><td>Jessica Davis</td><td>Fetal Heart Tones</td><td>Stable (140s)</td><td>2025-11-11</td></tr><tr><td>201</td><td>Srinivas</td><td>GFR</td><td>25 (null)</td><td>NA</td></tr><tr><td>201</td><td>Srinivas</td><td>Creatinine</td><td>2.8 mg/dL (null)</td><td>NA</td></tr><tr><td>202</td><td>Ranjan</td><td>Rapid Strep Test</td><td>Pending (null)</td><td>NA</td></tr><tr><td>203</td><td>Pardhu</td><td>MRI Lumbar Spine</td><td>Scheduled (null)</td><td>NA</td></tr><tr><td>205</td><td>Ramana</td><td>Hemoglobin</td><td>7.2 g/dL (Low) (null)</td><td>NA</td></tr><tr><td>205</td><td>Ramana</td><td>Type and Cross</td><td>Ordered (2 units PRBC) (null)</td><td>NA</td></tr><tr><td>209</td><td>Pataan</td><td>TSH</td><td>12.5 mIU/L (High) (null)</td><td>NA</td></tr><tr><td>209</td><td>Pataan</td><td>TSH</td><td>Repeat ordered in 6 weeks (null)</td><td>NA</td></tr><tr><td>210</td><td>Maddy</td><td>Ankle X-ray</td><td>Pending (null)</td><td>NA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "John Doe",
         "Troponin",
         "Elevated",
         "2025-11-10"
        ],
        [
         101,
         "John Doe",
         "ECG",
         "ST-segment elevation",
         "2025-11-10"
        ],
        [
         101,
         "John Doe",
         "BNP",
         "High",
         "2025-11-11"
        ],
        [
         102,
         "Jane Smith",
         "X-Ray (Right Hip)",
         "Femoral neck fracture",
         "2025-11-09"
        ],
        [
         102,
         "Jane Smith",
         "Hgb",
         "9.5 g/dL (Low)",
         "2025-11-10"
        ],
        [
         103,
         "Robert Brown",
         "Sputum Culture",
         "Pending",
         "2025-11-11"
        ],
        [
         103,
         "Robert Brown",
         "ABG",
         "Respiratory acidosis",
         "2025-11-11"
        ],
        [
         103,
         "Robert Brown",
         "Chest X-Ray",
         "Infiltrates in right lower lobe",
         "2025-11-10"
        ],
        [
         104,
         "Maria Garcia",
         "Blood Glucose",
         "450 mg/dL (High)",
         "2025-11-11"
        ],
        [
         104,
         "Maria Garcia",
         "Urinalysis",
         "Ketones present",
         "2025-11-11"
        ],
        [
         104,
         "Maria Garcia",
         "A1c",
         "11.2%",
         "2025-11-11"
        ],
        [
         105,
         "David Lee",
         "CT Head (Non-contrast)",
         "No acute bleed",
         "2025-11-08"
        ],
        [
         105,
         "David Lee",
         "MRI Brain",
         "Acute ischemic stroke, left MCA",
         "2025-11-09"
        ],
        [
         106,
         "Sarah Chen",
         "Amylase",
         "950 U/L (High)",
         "2025-11-10"
        ],
        [
         106,
         "Sarah Chen",
         "Lipase",
         "1200 U/L (High)",
         "2025-11-10"
        ],
        [
         107,
         "Michael Johnson",
         "WBC",
         "14.2 k/uL (High)",
         "2025-11-11"
        ],
        [
         107,
         "Michael Johnson",
         "Wound Culture",
         "Pending",
         "2025-11-11"
        ],
        [
         108,
         "Emily White",
         "RSV Swab",
         "Positive",
         "2025-11-10"
        ],
        [
         109,
         "Kevin Patel",
         "Urine Drug Screen",
         "Negative",
         "2025-11-09"
        ],
        [
         109,
         "Kevin Patel",
         "TSH",
         "Within normal limits",
         "2025-11-09"
        ],
        [
         110,
         "Jessica Davis",
         "GBS",
         "Positive",
         "2025-11-01"
        ],
        [
         110,
         "Jessica Davis",
         "Fetal Heart Tones",
         "Stable (140s)",
         "2025-11-11"
        ],
        [
         201,
         "Srinivas",
         "GFR",
         "25 (null)",
         "NA"
        ],
        [
         201,
         "Srinivas",
         "Creatinine",
         "2.8 mg/dL (null)",
         "NA"
        ],
        [
         202,
         "Ranjan",
         "Rapid Strep Test",
         "Pending (null)",
         "NA"
        ],
        [
         203,
         "Pardhu",
         "MRI Lumbar Spine",
         "Scheduled (null)",
         "NA"
        ],
        [
         205,
         "Ramana",
         "Hemoglobin",
         "7.2 g/dL (Low) (null)",
         "NA"
        ],
        [
         205,
         "Ramana",
         "Type and Cross",
         "Ordered (2 units PRBC) (null)",
         "NA"
        ],
        [
         209,
         "Pataan",
         "TSH",
         "12.5 mIU/L (High) (null)",
         "NA"
        ],
        [
         209,
         "Pataan",
         "TSH",
         "Repeat ordered in 6 weeks (null)",
         "NA"
        ],
        [
         210,
         "Maddy",
         "Ankle X-ray",
         "Pending (null)",
         "NA"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "test",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "result",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, split, regexp_extract, regexp_replace, trim, when\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "\n",
    "# Load your base DataFrame\n",
    "\n",
    "# OR if it's already a table:\n",
    "df = spark.read.table(\"patient_notes\")\n",
    "\n",
    "# =========================================================\n",
    "# PREPROCESSING: CREATE LAB RESULTS SILVER TABLE\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n--- Creating Silver Table: Lab Results ---\")\n",
    "\n",
    "# --- Step 1: Split the lab_results by pipe (|) to create an array ---\n",
    "df_split = df.withColumn(\n",
    "    \"lab_results_array\",\n",
    "    split(col(\"lab_results\"), \"\\\\|\")\n",
    ")\n",
    "\n",
    "# --- Step 2: Explode the array to get one row per lab result ---\n",
    "df_exploded = df_split.withColumn(\n",
    "    \"lab_result_raw\",\n",
    "    explode(col(\"lab_results_array\"))\n",
    ").filter(col(\"lab_result_raw\").isNotNull())\n",
    "\n",
    "# --- Step 3: Parse each lab result string ---\n",
    "# Format: \"Test: Result (Date)\" where date is in YYYY-MM-DD format\n",
    "lab_results_silver_df = df_exploded.select(\n",
    "    col(\"patient_id\"),\n",
    "    col(\"patient_name\"),\n",
    "    # Extract test name (everything before the colon)\n",
    "    trim(regexp_extract(col(\"lab_result_raw\"), \"^([^:]+):\", 1)).alias(\"test\"),\n",
    "    # Extract result (everything between colon and date pattern, or end of string)\n",
    "    # Remove the date pattern at the end if it exists\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_extract(col(\"lab_result_raw\"), \":\\\\s*(.+)\", 1),\n",
    "            \"\\\\s*\\\\(\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\)\\\\s*$\",\n",
    "            \"\"\n",
    "        )\n",
    "    ).alias(\"result\"),\n",
    "    # Extract date ONLY if it matches YYYY-MM-DD pattern in parentheses\n",
    "    when(\n",
    "        regexp_extract(col(\"lab_result_raw\"), \"\\\\((\\\\d{4}-\\\\d{2}-\\\\d{2})\\\\)\", 1) != \"\",\n",
    "        regexp_extract(col(\"lab_result_raw\"), \"\\\\((\\\\d{4}-\\\\d{2}-\\\\d{2})\\\\)\", 1)\n",
    "    ).otherwise(\"NA\").alias(\"date\")\n",
    ").filter(col(\"test\") != \"\")\n",
    "\n",
    "# Show the result\n",
    "print(\"\\n--- Lab Results Silver Table ---\")\n",
    "lab_results_silver_df.display(50, truncate=False)\n",
    "\n",
    "# Optional: Write to Delta table\n",
    "# lab_results_silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lab_results_silver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57343100-2ad6-424f-9f14-5181eaab31f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transcripts results you just created\n",
    "transcripts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"transcripts_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f699a7f5-4554-47ff-bf03-971a51cab589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the lab results you just created\n",
    "lab_results_silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lab_results_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89baa3d-fd0a-4abd-b08f-7d0c5ddbab68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the nursing notes from the AI (QA) model\n",
    "# (You may need to re-run the cell that creates 'ai_nursing_silver_df' first)\n",
    "ai_nursing_silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"nursing_notes_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff79768e-0f61-4e2b-a8af-5f0a007296fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Gold Layer Pre-processing Complete (SOAP Format) ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_name</th><th>Full_History_Text</th></tr></thead><tbody><tr><td>101</td><td>John Doe</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "chest pain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Michael 107, subject to post-operation day 3, appendectomy, reports worsening incision pain and fever, objective fever of 102, white count 14.2, incision site is red, warm and draining fluid, assessment, surgical site infection, plan send wood culture, start bankomacinal, consult surgery.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Troponin: Elevated | ECG: ST-segment elevation | BNP: High\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "8/10 chest pain\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Family anxious</td></tr><tr><td>102</td><td>Jane Smith</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "pain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Jessica, subject 32 year old female, third and weak pregnant in active labor, contractions every 3 to 5 minutes, object 2, fetal heart tones, stable, patient is 6 centimeter dilated, 100% effaced, GBS positive, assessment, active labor, plan, admit to L and D, start Penthaline G for GBS, notice Anastasia.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "X-Ray (Right Hip): Femoral neck fracture | Hgb: 9.5 g/dL (Low)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Vitals stable\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "N/A</td></tr><tr><td>103</td><td>Robert Brown</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "cough and fever\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Patient John 101, subject to 58-year-old male history of hyperattention presents with severe crushing chest pain, objective TCG shows ST segment elevation, troponin is marked elevated BNB high assessment, acute myocardial infraction, plan admit to cardiac unit, consult cardiology.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Sputum Culture: Pending | ABG: Respiratory acidosis | Chest X-Ray: Infiltrates in right lower lobe\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "92%\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Started on IV antibiotics</td></tr><tr><td>104</td><td>Maria Garcia</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "DKA\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  David Onzerofel, subject to 67-year-old male, acute right-side weakness, symptoms started on our ego. Object to CT head is negative obliter, MRI confirms left MCA ishemic stroke, NIH score is 11th. Assessment, acute ishemic stroke, plan, administer TBA per protocol, and shorten neuro ICU.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Blood Glucose: 450 mg/dL (High) | Urinalysis: Ketones present | A1c: 11.2%\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Diabetes educator consulted\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Started on insulin drip per protocol</td></tr><tr><td>105</td><td>David Lee</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "aphasia\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Emily 108, subject 2, 6 month old female, with respiratory distress and poor feeding, objective, audible phasing and intercostal retractions, nasal swab is born to, for RSV, as a cement bronchialitis, secondary to RSV, plan, add me to pdiatrics, provide cool mist and nasal suctioning.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "CT Head (Non-contrast): No acute bleed | MRI Brain: Acute ischemic stroke, left MCA\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "NIH Stroke Scale: 12\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "NIH Stroke Scale: 12</td></tr><tr><td>106</td><td>Sarah Chen</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "acute-onset abdominal pain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Kevin 109 Subjective 29-year-old male admitted for acute depressive episode with pause-bass you suicidal ideation objective urine, dry, drug screen negative patient is cooperative but has a flat effect assessment major depressive disorder with suicidal ideation plan admit to psychiatrist start one to one observation\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Amylase: 950 U/L (High) | Lipase: 1200 U/L (High)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Receiving IV fluids and pain management\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Receiving IV fluids and pain management</td></tr><tr><td>107</td><td>Michael Johnson</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "infection\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  The patient is a fortifier year old male presenting with a three-day history of severe chest pain and shortness of breath. He describes the pain as a crushing pressure. He has a history of hypertension and type 2 diabetes. Vital strengths are stable. ECG shows ST segment elevation in the anterior leads, proponent levels are elevated. Patient is diagnosed with an acute myocondyl infection. We will admit the patient to the cardiac unit for continuous monitoring, we will start him on aspirin, lysinopryl and start him, we will also consult cardiology for an urgent cardia characterization.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "WBC: 14.2 k/uL (High) | Wound Culture: Pending\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "N/A</td></tr><tr><td>108</td><td>Emily White</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "respiratory distress\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Note 4 James Smith 102 Subjective 72-year-old female presented after a fall, severe right-hit pain unable to bear weight, objective, right leg shortened and externally rotated, X-ray confirmed femoral neck fracture, HGB is 9.5, assessment, accurate right-hit fracture, plan and pivot for surgery, or the consult for fixation and manage pain with PCA post-op.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "RSV Swab: Positive\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Suctioned for secretions PRN\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Suctioned for secretions PRN</td></tr><tr><td>109</td><td>Kevin Patel</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "suicidal ideation\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  Sara, 35 year old, female, acute, epigastric pain, radiating to back with nausea, objective analyze his 950, liposis 1200, assessment, acute pancreas, plan, start morphine, order right upper quadrant, ultrasound.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Urine Drug Screen: Negative | TSH: Within normal limits\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Safety plan</td></tr><tr><td>110</td><td>Jessica Davis</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "Patient is 39 weeks, admitted in active labor\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION:  For our brown 103 subject 68 year old male with a 3 day history of worsening cough fever 101.5 shortness of breath objective o2 saturation 92% on 2 liters chest x ray right lower low infiltrates abg shows respiratory acidosis assessment pneumonia plan admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen.\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "GBS: Positive | Fetal Heart Tones: Stable (140s)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Epidural placed\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Receiving Penicillin G for GBS prophylaxis</td></tr><tr><td>201</td><td>Srinivas</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "fatigue\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: BP 160/90\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "GFR: 25 (null) | Creatinine: 2.8 mg/dL (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Stage 4 Chronic Kidney Disease\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "N/A</td></tr><tr><td>202</td><td>Ranjan</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "sore throat and fever\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: Temp 102\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Rapid Strep Test: Pending (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Rapid strep test\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Will start penicillin</td></tr><tr><td>203</td><td>Pardhu</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "lower back pain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "MRI Lumbar Spine: Scheduled (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Suspected herniated disc with sciatica\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "lumbar spine MRI</td></tr><tr><td>205</td><td>Ramana</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "dizziness\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Hemoglobin: 7.2 g/dL (Low) (null) | Type and Cross: Ordered (2 units PRBC) (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Severe Anemia\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Admitting to hospital for 2 units packed red blood cell transfusion</td></tr><tr><td>206</td><td>Vinuthana</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "burning chest pain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "GERD\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Started on daily omeprazole trial</td></tr><tr><td>207</td><td>Akanksha</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "difficulty breathing\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Transferring to ER\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Transferring to ER for further monitoring</td></tr><tr><td>208</td><td>Roy</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "panic attacks\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "GAD\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "States symptoms are interfering with her work</td></tr><tr><td>209</td><td>Pataan</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "weight gain\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "TSH: 12.5 mIU/L (High) (null) | TSH: Repeat ordered in 6 weeks (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "weight gain\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Follow-up TSH lab scheduled for 6 weeks</td></tr><tr><td>210</td><td>Maddy</td><td>--- SUBJECTIVE ---\n",
       "\n",
       "Suspect fracture\n",
       "\n",
       "--- OBJECTIVE ---\n",
       "\n",
       "DOCTOR'S DICTATION: N/A\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Ankle X-ray: Pending (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "N/A\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "RICE protocol initiated</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "John Doe",
         "--- SUBJECTIVE ---\n\nchest pain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Michael 107, subject to post-operation day 3, appendectomy, reports worsening incision pain and fever, objective fever of 102, white count 14.2, incision site is red, warm and draining fluid, assessment, surgical site infection, plan send wood culture, start bankomacinal, consult surgery.\nVITALS: N/A\n\n--- KEY LABS ---\n\nTroponin: Elevated | ECG: ST-segment elevation | BNP: High\n\n--- ASSESSMENT ---\n\n8/10 chest pain\n\n--- PLAN ---\n\nFamily anxious"
        ],
        [
         102,
         "Jane Smith",
         "--- SUBJECTIVE ---\n\npain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Jessica, subject 32 year old female, third and weak pregnant in active labor, contractions every 3 to 5 minutes, object 2, fetal heart tones, stable, patient is 6 centimeter dilated, 100% effaced, GBS positive, assessment, active labor, plan, admit to L and D, start Penthaline G for GBS, notice Anastasia.\nVITALS: N/A\n\n--- KEY LABS ---\n\nX-Ray (Right Hip): Femoral neck fracture | Hgb: 9.5 g/dL (Low)\n\n--- ASSESSMENT ---\n\nVitals stable\n\n--- PLAN ---\n\nN/A"
        ],
        [
         103,
         "Robert Brown",
         "--- SUBJECTIVE ---\n\ncough and fever\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Patient John 101, subject to 58-year-old male history of hyperattention presents with severe crushing chest pain, objective TCG shows ST segment elevation, troponin is marked elevated BNB high assessment, acute myocardial infraction, plan admit to cardiac unit, consult cardiology.\nVITALS: N/A\n\n--- KEY LABS ---\n\nSputum Culture: Pending | ABG: Respiratory acidosis | Chest X-Ray: Infiltrates in right lower lobe\n\n--- ASSESSMENT ---\n\n92%\n\n--- PLAN ---\n\nStarted on IV antibiotics"
        ],
        [
         104,
         "Maria Garcia",
         "--- SUBJECTIVE ---\n\nDKA\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  David Onzerofel, subject to 67-year-old male, acute right-side weakness, symptoms started on our ego. Object to CT head is negative obliter, MRI confirms left MCA ishemic stroke, NIH score is 11th. Assessment, acute ishemic stroke, plan, administer TBA per protocol, and shorten neuro ICU.\nVITALS: N/A\n\n--- KEY LABS ---\n\nBlood Glucose: 450 mg/dL (High) | Urinalysis: Ketones present | A1c: 11.2%\n\n--- ASSESSMENT ---\n\nDiabetes educator consulted\n\n--- PLAN ---\n\nStarted on insulin drip per protocol"
        ],
        [
         105,
         "David Lee",
         "--- SUBJECTIVE ---\n\naphasia\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Emily 108, subject 2, 6 month old female, with respiratory distress and poor feeding, objective, audible phasing and intercostal retractions, nasal swab is born to, for RSV, as a cement bronchialitis, secondary to RSV, plan, add me to pdiatrics, provide cool mist and nasal suctioning.\nVITALS: N/A\n\n--- KEY LABS ---\n\nCT Head (Non-contrast): No acute bleed | MRI Brain: Acute ischemic stroke, left MCA\n\n--- ASSESSMENT ---\n\nNIH Stroke Scale: 12\n\n--- PLAN ---\n\nNIH Stroke Scale: 12"
        ],
        [
         106,
         "Sarah Chen",
         "--- SUBJECTIVE ---\n\nacute-onset abdominal pain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Kevin 109 Subjective 29-year-old male admitted for acute depressive episode with pause-bass you suicidal ideation objective urine, dry, drug screen negative patient is cooperative but has a flat effect assessment major depressive disorder with suicidal ideation plan admit to psychiatrist start one to one observation\nVITALS: N/A\n\n--- KEY LABS ---\n\nAmylase: 950 U/L (High) | Lipase: 1200 U/L (High)\n\n--- ASSESSMENT ---\n\nReceiving IV fluids and pain management\n\n--- PLAN ---\n\nReceiving IV fluids and pain management"
        ],
        [
         107,
         "Michael Johnson",
         "--- SUBJECTIVE ---\n\ninfection\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  The patient is a fortifier year old male presenting with a three-day history of severe chest pain and shortness of breath. He describes the pain as a crushing pressure. He has a history of hypertension and type 2 diabetes. Vital strengths are stable. ECG shows ST segment elevation in the anterior leads, proponent levels are elevated. Patient is diagnosed with an acute myocondyl infection. We will admit the patient to the cardiac unit for continuous monitoring, we will start him on aspirin, lysinopryl and start him, we will also consult cardiology for an urgent cardia characterization.\nVITALS: N/A\n\n--- KEY LABS ---\n\nWBC: 14.2 k/uL (High) | Wound Culture: Pending\n\n--- ASSESSMENT ---\n\nN/A\n\n--- PLAN ---\n\nN/A"
        ],
        [
         108,
         "Emily White",
         "--- SUBJECTIVE ---\n\nrespiratory distress\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Note 4 James Smith 102 Subjective 72-year-old female presented after a fall, severe right-hit pain unable to bear weight, objective, right leg shortened and externally rotated, X-ray confirmed femoral neck fracture, HGB is 9.5, assessment, accurate right-hit fracture, plan and pivot for surgery, or the consult for fixation and manage pain with PCA post-op.\nVITALS: N/A\n\n--- KEY LABS ---\n\nRSV Swab: Positive\n\n--- ASSESSMENT ---\n\nSuctioned for secretions PRN\n\n--- PLAN ---\n\nSuctioned for secretions PRN"
        ],
        [
         109,
         "Kevin Patel",
         "--- SUBJECTIVE ---\n\nsuicidal ideation\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  Sara, 35 year old, female, acute, epigastric pain, radiating to back with nausea, objective analyze his 950, liposis 1200, assessment, acute pancreas, plan, start morphine, order right upper quadrant, ultrasound.\nVITALS: N/A\n\n--- KEY LABS ---\n\nUrine Drug Screen: Negative | TSH: Within normal limits\n\n--- ASSESSMENT ---\n\nN/A\n\n--- PLAN ---\n\nSafety plan"
        ],
        [
         110,
         "Jessica Davis",
         "--- SUBJECTIVE ---\n\nPatient is 39 weeks, admitted in active labor\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION:  For our brown 103 subject 68 year old male with a 3 day history of worsening cough fever 101.5 shortness of breath objective o2 saturation 92% on 2 liters chest x ray right lower low infiltrates abg shows respiratory acidosis assessment pneumonia plan admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen.\nVITALS: N/A\n\n--- KEY LABS ---\n\nGBS: Positive | Fetal Heart Tones: Stable (140s)\n\n--- ASSESSMENT ---\n\nEpidural placed\n\n--- PLAN ---\n\nReceiving Penicillin G for GBS prophylaxis"
        ],
        [
         201,
         "Srinivas",
         "--- SUBJECTIVE ---\n\nfatigue\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: BP 160/90\n\n--- KEY LABS ---\n\nGFR: 25 (null) | Creatinine: 2.8 mg/dL (null)\n\n--- ASSESSMENT ---\n\nStage 4 Chronic Kidney Disease\n\n--- PLAN ---\n\nN/A"
        ],
        [
         202,
         "Ranjan",
         "--- SUBJECTIVE ---\n\nsore throat and fever\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: Temp 102\n\n--- KEY LABS ---\n\nRapid Strep Test: Pending (null)\n\n--- ASSESSMENT ---\n\nRapid strep test\n\n--- PLAN ---\n\nWill start penicillin"
        ],
        [
         203,
         "Pardhu",
         "--- SUBJECTIVE ---\n\nlower back pain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nMRI Lumbar Spine: Scheduled (null)\n\n--- ASSESSMENT ---\n\nSuspected herniated disc with sciatica\n\n--- PLAN ---\n\nlumbar spine MRI"
        ],
        [
         205,
         "Ramana",
         "--- SUBJECTIVE ---\n\ndizziness\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nHemoglobin: 7.2 g/dL (Low) (null) | Type and Cross: Ordered (2 units PRBC) (null)\n\n--- ASSESSMENT ---\n\nSevere Anemia\n\n--- PLAN ---\n\nAdmitting to hospital for 2 units packed red blood cell transfusion"
        ],
        [
         206,
         "Vinuthana",
         "--- SUBJECTIVE ---\n\nburning chest pain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nN/A\n\n--- ASSESSMENT ---\n\nGERD\n\n--- PLAN ---\n\nStarted on daily omeprazole trial"
        ],
        [
         207,
         "Akanksha",
         "--- SUBJECTIVE ---\n\ndifficulty breathing\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nN/A\n\n--- ASSESSMENT ---\n\nTransferring to ER\n\n--- PLAN ---\n\nTransferring to ER for further monitoring"
        ],
        [
         208,
         "Roy",
         "--- SUBJECTIVE ---\n\npanic attacks\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nN/A\n\n--- ASSESSMENT ---\n\nGAD\n\n--- PLAN ---\n\nStates symptoms are interfering with her work"
        ],
        [
         209,
         "Pataan",
         "--- SUBJECTIVE ---\n\nweight gain\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nTSH: 12.5 mIU/L (High) (null) | TSH: Repeat ordered in 6 weeks (null)\n\n--- ASSESSMENT ---\n\nweight gain\n\n--- PLAN ---\n\nFollow-up TSH lab scheduled for 6 weeks"
        ],
        [
         210,
         "Maddy",
         "--- SUBJECTIVE ---\n\nSuspect fracture\n\n--- OBJECTIVE ---\n\nDOCTOR'S DICTATION: N/A\nVITALS: N/A\n\n--- KEY LABS ---\n\nAnkle X-ray: Pending (null)\n\n--- ASSESSMENT ---\n\nN/A\n\n--- PLAN ---\n\nRICE protocol initiated"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Full_History_Text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, lit, collect_list, concat_ws, coalesce\n",
    "\n",
    "# --- 1. Load All Our Silver Tables ---\n",
    "labs_df = spark.read.table(\"lab_results_silver\")\n",
    "nursing_df = spark.read.table(\"nursing_notes_silver\") # This table is now correct\n",
    "transcripts_df = spark.read.table(\"transcripts_bronze\")\n",
    "patients_df = spark.read.table(\"patient_notes\").select(\"patient_id\", \"patient_name\")\n",
    "\n",
    "# --- 2. Aggregate Lab Results ---\n",
    "labs_with_string = labs_df.withColumn(\n",
    "    \"lab_string\", \n",
    "    concat(col(\"test\"), lit(\": \"), col(\"result\"))\n",
    ")\n",
    "labs_agg_df = labs_with_string.groupBy(\"patient_id\").agg(\n",
    "    concat_ws(\" | \", collect_list(\"lab_string\")).alias(\"lab_summary\")\n",
    ")\n",
    "\n",
    "# --- 3. Join All Data Sources ---\n",
    "final_join_df = patients_df.join(\n",
    "    transcripts_df.select(\"patient_id\", \"transcript_text\"), \"patient_id\", \"left\"\n",
    ").join(\n",
    "    nursing_df, \"patient_id\", \"left\"\n",
    ").join(\n",
    "    labs_agg_df, \"patient_id\", \"left\"\n",
    ")\n",
    "\n",
    "# --- 4. Create the Final SOAP Note Prompt ---\n",
    "# *** MODIFIED: Removed 'other_clinical_details' line ***\n",
    "soap_prompt_df = final_join_df.withColumn(\n",
    "    \"Full_History_Text\",\n",
    "    concat_ws(\n",
    "        \"\\n\\n\",  # Separate each section with a double newline\n",
    "        \n",
    "        lit(\"--- SUBJECTIVE ---\"),\n",
    "        coalesce(col(\"patient_problem\"), lit(\"N/A\")),\n",
    "        \n",
    "        lit(\"--- OBJECTIVE ---\"),\n",
    "        concat(\n",
    "            lit(\"DOCTOR'S DICTATION: \"), coalesce(col(\"transcript_text\"), lit(\"N/A\")),\n",
    "            lit(\"\\nVITALS: \"), coalesce(col(\"vitals\"), lit(\"N/A\"))\n",
    "        ),\n",
    "        \n",
    "        lit(\"--- KEY LABS ---\"),\n",
    "        coalesce(col(\"lab_summary\"), lit(\"N/A\")),\n",
    "        \n",
    "        lit(\"--- ASSESSMENT ---\"),\n",
    "        coalesce(col(\"assessment\"), lit(\"N/A\")),\n",
    "        \n",
    "        lit(\"--- PLAN ---\"),\n",
    "        coalesce(col(\"plan\"), lit(\"N/A\"))\n",
    "    )\n",
    ").select(\"patient_id\", \"patient_name\", \"Full_History_Text\")\n",
    "\n",
    "# --- 5. Show the Final Result ---\n",
    "print(\"\\n--- Gold Layer Pre-processing Complete (SOAP Format) ---\")\n",
    "display(soap_prompt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc2787a-ecac-4095-ac5d-0fc26f412127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 'discharge_summary_gold' table.\n"
     ]
    }
   ],
   "source": [
    "soap_prompt_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"discharge_summary_gold\")\n",
    "\n",
    "print(\"Successfully saved 'discharge_summary_gold' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a48b893-10c6-4611-93f3-3f34d10a4e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Initializing AI Models for Patient Use Case (on CPU) ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\nDevice set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AI Models Initialized ---\nCollecting data to process for patient portal...\n > Processed Patient ID: 101\n > Processed Patient ID: 102\n > Processed Patient ID: 103\n > Processed Patient ID: 104\n > Processed Patient ID: 105\n > Processed Patient ID: 106\n > Processed Patient ID: 107\n > Processed Patient ID: 108\n > Processed Patient ID: 109\n > Processed Patient ID: 110\n > Processed Patient ID: 201\n > Processed Patient ID: 202\n > Processed Patient ID: 203\n > Processed Patient ID: 205\n > Processed Patient ID: 206\n > Processed Patient ID: 207\n > Processed Patient ID: 208\n > Processed Patient ID: 209\n > Processed Patient ID: 210\n\n--- Final 'Patient Portal' Table ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>doctor_advice</th><th>key_terms</th><th>patient_id</th><th>patient_name</th></tr></thead><tbody><tr><td>assessment | send wood culture</td><td></td><td>101</td><td>John Doe</td></tr><tr><td>admit to L and D | assessment, active labor, plan, admit to L and D</td><td>Medication: pen</td><td>102</td><td>Jane Smith</td></tr><tr><td>admit to cardiac unit, consult cardiology | consult cardiology</td><td>Medication: antibiotics</td><td>103</td><td>Robert Brown</td></tr><tr><td>administer TBA per protocol | administer TBA per protocol, and shorten neuro ICU</td><td>Medication: insulin</td><td>104</td><td>Maria Garcia</td></tr><tr><td>add me to pdiatrics | audible phasing and intercostal retractions</td><td></td><td>105</td><td>David Lee</td></tr><tr><td>admit to psychiatrist start one to one observation | cooperative</td><td></td><td>106</td><td>Sarah Chen</td></tr><tr><td></td><td>Medication: ##pryl | Medication: lysino</td><td>107</td><td>Michael Johnson</td></tr><tr><td>consult for fixation and manage pain with PCA post-op | plan and pivot for surgery</td><td>Medication: secret</td><td>108</td><td>Emily White</td></tr><tr><td>Safety plan | objective analyze his 950, liposis 1200, assessment</td><td></td><td>109</td><td>Kevin Patel</td></tr><tr><td>continue oxygen</td><td>Medication: ##tromy | Medication: ##zy | Medication: eggs | Medication: penicillin</td><td>110</td><td>Jessica Davis</td></tr><tr><td></td><td></td><td>201</td><td>Srinivas</td></tr><tr><td>Will start penicillin</td><td>Medication: ##icillin | Medication: pen</td><td>202</td><td>Ranjan</td></tr><tr><td></td><td></td><td>203</td><td>Pardhu</td></tr><tr><td>Type and Cross: Ordered</td><td></td><td>205</td><td>Ramana</td></tr><tr><td>Started on daily omeprazole trial</td><td>Medication: omeprazole</td><td>206</td><td>Vinuthana</td></tr><tr><td>difficulty breathing | further monitoring</td><td></td><td>207</td><td>Akanksha</td></tr><tr><td>interfering with her work | symptoms are interfering with her work</td><td></td><td>208</td><td>Roy</td></tr><tr><td>Repeat | scheduled for 6 weeks</td><td></td><td>209</td><td>Pataan</td></tr><tr><td>protocol</td><td></td><td>210</td><td>Maddy</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "assessment | send wood culture",
         "",
         101,
         "John Doe"
        ],
        [
         "admit to L and D | assessment, active labor, plan, admit to L and D",
         "Medication: pen",
         102,
         "Jane Smith"
        ],
        [
         "admit to cardiac unit, consult cardiology | consult cardiology",
         "Medication: antibiotics",
         103,
         "Robert Brown"
        ],
        [
         "administer TBA per protocol | administer TBA per protocol, and shorten neuro ICU",
         "Medication: insulin",
         104,
         "Maria Garcia"
        ],
        [
         "add me to pdiatrics | audible phasing and intercostal retractions",
         "",
         105,
         "David Lee"
        ],
        [
         "admit to psychiatrist start one to one observation | cooperative",
         "",
         106,
         "Sarah Chen"
        ],
        [
         "",
         "Medication: ##pryl | Medication: lysino",
         107,
         "Michael Johnson"
        ],
        [
         "consult for fixation and manage pain with PCA post-op | plan and pivot for surgery",
         "Medication: secret",
         108,
         "Emily White"
        ],
        [
         "Safety plan | objective analyze his 950, liposis 1200, assessment",
         "",
         109,
         "Kevin Patel"
        ],
        [
         "continue oxygen",
         "Medication: ##tromy | Medication: ##zy | Medication: eggs | Medication: penicillin",
         110,
         "Jessica Davis"
        ],
        [
         "",
         "",
         201,
         "Srinivas"
        ],
        [
         "Will start penicillin",
         "Medication: ##icillin | Medication: pen",
         202,
         "Ranjan"
        ],
        [
         "",
         "",
         203,
         "Pardhu"
        ],
        [
         "Type and Cross: Ordered",
         "",
         205,
         "Ramana"
        ],
        [
         "Started on daily omeprazole trial",
         "Medication: omeprazole",
         206,
         "Vinuthana"
        ],
        [
         "difficulty breathing | further monitoring",
         "",
         207,
         "Akanksha"
        ],
        [
         "interfering with her work | symptoms are interfering with her work",
         "",
         208,
         "Roy"
        ],
        [
         "Repeat | scheduled for 6 weeks",
         "",
         209,
         "Pataan"
        ],
        [
         "protocol",
         "",
         210,
         "Maddy"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "doctor_advice",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key_terms",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- 1. Load the Data We Need ---\n",
    "# This is the DataFrame we created in the previous step\n",
    "# It has patient_id, patient_name, and Full_History_Text\n",
    "df_for_patient = spark.read.table(\"discharge_summary_gold\") # Or use soap_prompt_df if it's still in memory\n",
    "\n",
    "# --- 2. Initialize the AI Models ---\n",
    "print(\"\\n--- Initializing AI Models for Patient Use Case (on CPU) ---\")\n",
    "\n",
    "# Model 1: Named Entity Recognition (NER) for \"Knowledge\"\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"d4data/biomedical-ner-all\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "\n",
    "# Model 2: Question Answering (QA) for \"Action\"\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\",\n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "print(\"--- AI Models Initialized ---\")\n",
    "\n",
    "# --- 3. Collect Data to Process ---\n",
    "print(\"Collecting data to process for patient portal...\")\n",
    "patients_to_process = df_for_patient.collect()\n",
    "\n",
    "# --- 4. Define the \"Action\" Questions ---\n",
    "action_questions = {\n",
    "    \"doctor_advice\": \"What advice or instructions did the doctor give the patient?\",\n",
    "    \"follow_up\": \"What is the follow-up plan?\"\n",
    "}\n",
    "\n",
    "# --- 5. Loop, Extract Knowledge & Action ---\n",
    "patient_portal_data = []\n",
    "\n",
    "for row in patients_to_process:\n",
    "    patient_id = row[\"patient_id\"]\n",
    "    patient_name = row[\"patient_name\"]\n",
    "    full_text = row[\"Full_History_Text\"]\n",
    "    \n",
    "    result = {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"patient_name\": patient_name\n",
    "    }\n",
    "    \n",
    "    # --- Part 1: \"Knowledge\" (NER) ---\n",
    "    ner_results = ner_pipeline(full_text)\n",
    "    \n",
    "    key_terms = []\n",
    "    for entity in ner_results:\n",
    "        entity_type = entity['entity_group']\n",
    "        entity_word = entity['word'].strip().lower() # Clean up the word\n",
    "        \n",
    "        # We only want to show the patient the most important terms\n",
    "        if entity_type in [\"Medication\", \"Disease\", \"Symptom\", \"Treatment\"]:\n",
    "            if len(entity_word) > 2 and \"n/a\" not in entity_word:\n",
    "                key_terms.append(f\"{entity_type}: {entity_word}\")\n",
    "    \n",
    "    # Get a unique, sorted list\n",
    "    result[\"key_terms\"] = \" | \".join(sorted(list(set(key_terms))))\n",
    "\n",
    "    # --- Part 2: \"Action\" (QA) ---\n",
    "    advice_list = []\n",
    "    for col_name, question in action_questions.items():\n",
    "        qa_result = qa_pipeline(question=question, context=full_text)\n",
    "        \n",
    "        if qa_result['score'] > 0.05: # Lower threshold to catch more advice\n",
    "            # Clean the answer\n",
    "            answer = qa_result['answer'].strip(\" .\")\n",
    "            if \"N/A\" not in answer:\n",
    "                advice_list.append(answer)\n",
    "    \n",
    "    result[\"doctor_advice\"] = \" | \".join(sorted(list(set(advice_list))))\n",
    "            \n",
    "    patient_portal_data.append(result)\n",
    "    print(f\" > Processed Patient ID: {patient_id}\")\n",
    "\n",
    "# --- 6. Create the Final \"Patient Portal\" DataFrame ---\n",
    "patient_portal_df = spark.createDataFrame(patient_portal_data)\n",
    "\n",
    "print(\"\\n--- Final 'Patient Portal' Table ---\")\n",
    "display(patient_portal_df)\n",
    "\n",
    "# --- 7. Save Your Final Table ---\n",
    "patient_portal_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"patient_portal_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231e5440-5fd7-4a40-aa61-ca16041641d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Initializing AI Models for Patient Portal ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\nDevice set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AI Models Initialized ---\nCollecting patient data...\n\n--- Processing Patient: 101 - John Doe ---\n  ✓ Completed processing for Patient ID: 101\n\n--- Processing Patient: 102 - Jane Smith ---\n  ✓ Completed processing for Patient ID: 102\n\n--- Processing Patient: 103 - Robert Brown ---\n  ✓ Completed processing for Patient ID: 103\n\n--- Processing Patient: 104 - Maria Garcia ---\n  ✓ Completed processing for Patient ID: 104\n\n--- Processing Patient: 105 - David Lee ---\n  ✓ Completed processing for Patient ID: 105\n\n--- Processing Patient: 106 - Sarah Chen ---\n  ✓ Completed processing for Patient ID: 106\n\n--- Processing Patient: 107 - Michael Johnson ---\n  ✓ Completed processing for Patient ID: 107\n\n--- Processing Patient: 108 - Emily White ---\n  ✓ Completed processing for Patient ID: 108\n\n--- Processing Patient: 109 - Kevin Patel ---\n  ✓ Completed processing for Patient ID: 109\n\n--- Processing Patient: 110 - Jessica Davis ---\n  ✓ Completed processing for Patient ID: 110\n\n--- Processing Patient: 201 - Srinivas ---\n  ✓ Completed processing for Patient ID: 201\n\n--- Processing Patient: 202 - Ranjan ---\n  ✓ Completed processing for Patient ID: 202\n\n--- Processing Patient: 203 - Pardhu ---\n  ✓ Completed processing for Patient ID: 203\n\n--- Processing Patient: 205 - Ramana ---\n  ✓ Completed processing for Patient ID: 205\n\n--- Processing Patient: 206 - Vinuthana ---\n  ✓ Completed processing for Patient ID: 206\n\n--- Processing Patient: 207 - Akanksha ---\n  ✓ Completed processing for Patient ID: 207\n\n--- Processing Patient: 208 - Roy ---\n  ✓ Completed processing for Patient ID: 208\n\n--- Processing Patient: 209 - Pataan ---\n  ✓ Completed processing for Patient ID: 209\n\n--- Processing Patient: 210 - Maddy ---\n  ✓ Completed processing for Patient ID: 210\n\n================================================================================\nPATIENT PORTAL - FINAL OUTPUT\n================================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>patient_name</th><th>medications_prescribed</th><th>conditions_identified</th><th>symptoms_noted</th><th>doctor_instructions</th><th>follow_up_actions</th></tr></thead><tbody><tr><td>101</td><td>John Doe</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Family anxious | send wood culture, start bankomacinal, consult surgery</td><td>surgery</td></tr><tr><td>102</td><td>Jane Smith</td><td>pen</td><td>fracture</td><td>None documented</td><td>Please consult your discharge paperwork for detailed instructions</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>103</td><td>Robert Brown</td><td>antibiotics</td><td>respiratory acid</td><td>None documented</td><td>admit to cardiac unit, consult cardiology | ---\n",
       "\n",
       "Started on IV antibiotics</td><td>cardiology</td></tr><tr><td>104</td><td>Maria Garcia</td><td>insulin</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Started on insulin drip per protocol</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>105</td><td>David Lee</td><td>None documented</td><td>has</td><td>None documented</td><td>---\n",
       "\n",
       "NIH Stroke Scale: 12</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>106</td><td>Sarah Chen</td><td>None documented</td><td>depressive episode, major depressive disorder</td><td>None documented</td><td>admit to psychiatrist start one to one observation\n",
       "VITALS: N/A\n",
       "\n",
       "--- KEY LABS ---\n",
       "\n",
       "Amylase: 950 U/L (High) | ---\n",
       "\n",
       "Receiving IV fluids and pain management</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>107</td><td>Michael Johnson</td><td>lysino, pryl</td><td>ocond</td><td>None documented</td><td>Please consult your discharge paperwork for detailed instructions</td><td>cardiology for an urgent cardia characterization</td></tr><tr><td>108</td><td>Emily White</td><td>secret</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Suctioned for secretions PRN | and pivot for surgery, or the consult for fixation and manage pain with PCA post-op</td><td>for fixation and manage pain with PCA post-op</td></tr><tr><td>109</td><td>Kevin Patel</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Safety plan</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>110</td><td>Jessica Davis</td><td>eggs, penicillin, tromy</td><td>None documented</td><td>None documented</td><td>admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen | ---\n",
       "\n",
       "Receiving Penicillin G for GBS prophylaxis</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>201</td><td>Srinivas</td><td>None documented</td><td>kidney disease</td><td>None documented</td><td>Please consult your discharge paperwork for detailed instructions</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>202</td><td>Ranjan</td><td>icillin, pen</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Will start penicillin</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>203</td><td>Pardhu</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "lumbar spine MRI</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>205</td><td>Ramana</td><td>None documented</td><td>None documented</td><td>None documented</td><td>(2 units PRBC) (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "Severe Anemia\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Admitting to hospital for 2 units packed red blood cell transfusion</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>206</td><td>Vinuthana</td><td>omeprazole</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Started on daily omeprazole trial</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>207</td><td>Akanksha</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "Transferring to ER for further monitoring</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>208</td><td>Roy</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "States symptoms are interfering with her work</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr><tr><td>209</td><td>Pataan</td><td>None documented</td><td>None documented</td><td>None documented</td><td>in 6 weeks (null)\n",
       "\n",
       "--- ASSESSMENT ---\n",
       "\n",
       "weight gain\n",
       "\n",
       "--- PLAN ---\n",
       "\n",
       "Follow-up TSH lab scheduled for 6 weeks</td><td>6 weeks | TSH lab scheduled for 6 weeks</td></tr><tr><td>210</td><td>Maddy</td><td>None documented</td><td>None documented</td><td>None documented</td><td>---\n",
       "\n",
       "RICE protocol initiated</td><td>No specific follow-up documented. Contact your provider if symptoms worsen.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John Doe",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nFamily anxious | send wood culture, start bankomacinal, consult surgery",
         "surgery"
        ],
        [
         "102",
         "Jane Smith",
         "pen",
         "fracture",
         "None documented",
         "Please consult your discharge paperwork for detailed instructions",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "103",
         "Robert Brown",
         "antibiotics",
         "respiratory acid",
         "None documented",
         "admit to cardiac unit, consult cardiology | ---\n\nStarted on IV antibiotics",
         "cardiology"
        ],
        [
         "104",
         "Maria Garcia",
         "insulin",
         "None documented",
         "None documented",
         "---\n\nStarted on insulin drip per protocol",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "105",
         "David Lee",
         "None documented",
         "has",
         "None documented",
         "---\n\nNIH Stroke Scale: 12",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "106",
         "Sarah Chen",
         "None documented",
         "depressive episode, major depressive disorder",
         "None documented",
         "admit to psychiatrist start one to one observation\nVITALS: N/A\n\n--- KEY LABS ---\n\nAmylase: 950 U/L (High) | ---\n\nReceiving IV fluids and pain management",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "107",
         "Michael Johnson",
         "lysino, pryl",
         "ocond",
         "None documented",
         "Please consult your discharge paperwork for detailed instructions",
         "cardiology for an urgent cardia characterization"
        ],
        [
         "108",
         "Emily White",
         "secret",
         "None documented",
         "None documented",
         "---\n\nSuctioned for secretions PRN | and pivot for surgery, or the consult for fixation and manage pain with PCA post-op",
         "for fixation and manage pain with PCA post-op"
        ],
        [
         "109",
         "Kevin Patel",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nSafety plan",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "110",
         "Jessica Davis",
         "eggs, penicillin, tromy",
         "None documented",
         "None documented",
         "admit to floor start 4, septary, eggsone and ezytromycin, continue oxygen | ---\n\nReceiving Penicillin G for GBS prophylaxis",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "201",
         "Srinivas",
         "None documented",
         "kidney disease",
         "None documented",
         "Please consult your discharge paperwork for detailed instructions",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "202",
         "Ranjan",
         "icillin, pen",
         "None documented",
         "None documented",
         "---\n\nWill start penicillin",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "203",
         "Pardhu",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nlumbar spine MRI",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "205",
         "Ramana",
         "None documented",
         "None documented",
         "None documented",
         "(2 units PRBC) (null)\n\n--- ASSESSMENT ---\n\nSevere Anemia\n\n--- PLAN ---\n\nAdmitting to hospital for 2 units packed red blood cell transfusion",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "206",
         "Vinuthana",
         "omeprazole",
         "None documented",
         "None documented",
         "---\n\nStarted on daily omeprazole trial",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "207",
         "Akanksha",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nTransferring to ER for further monitoring",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "208",
         "Roy",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nStates symptoms are interfering with her work",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ],
        [
         "209",
         "Pataan",
         "None documented",
         "None documented",
         "None documented",
         "in 6 weeks (null)\n\n--- ASSESSMENT ---\n\nweight gain\n\n--- PLAN ---\n\nFollow-up TSH lab scheduled for 6 weeks",
         "6 weeks | TSH lab scheduled for 6 weeks"
        ],
        [
         "210",
         "Maddy",
         "None documented",
         "None documented",
         "None documented",
         "---\n\nRICE protocol initiated",
         "No specific follow-up documented. Contact your provider if symptoms worsen."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "patient_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "medications_prescribed",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "conditions_identified",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "symptoms_noted",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doctor_instructions",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "follow_up_actions",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from transformers import pipeline\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "df_for_patient = spark.read.table(\"discharge_summary_gold\")\n",
    "\n",
    "# --- 2. Initialize AI Models ---\n",
    "print(\"\\n--- Initializing AI Models for Patient Portal ---\")\n",
    "\n",
    "# NER for extracting medical entities\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"d4data/biomedical-ner-all\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# Summarization for better advice extraction\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "print(\"--- AI Models Initialized ---\")\n",
    "\n",
    "# --- 3. Collect Data ---\n",
    "print(\"Collecting patient data...\")\n",
    "patients_to_process = df_for_patient.collect()\n",
    "\n",
    "# --- 4. Process Each Patient ---\n",
    "patient_portal_data = []\n",
    "\n",
    "for row in patients_to_process:\n",
    "    patient_id = row[\"patient_id\"]\n",
    "    patient_name = row[\"patient_name\"]\n",
    "    full_text = row[\"Full_History_Text\"]\n",
    "    \n",
    "    print(f\"\\n--- Processing Patient: {patient_id} - {patient_name} ---\")\n",
    "    \n",
    "    # Initialize result dictionary with proper column order\n",
    "    result = {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"patient_name\": patient_name,\n",
    "        \"medications_prescribed\": \"\",\n",
    "        \"conditions_identified\": \"\",\n",
    "        \"symptoms_noted\": \"\",\n",
    "        \"doctor_instructions\": \"\",\n",
    "        \"follow_up_actions\": \"\"\n",
    "    }\n",
    "    \n",
    "    # --- STEP 1: Extract Medical Entities with Better Handling ---\n",
    "    try:\n",
    "        ner_results = ner_pipeline(full_text)\n",
    "        \n",
    "        medications = []\n",
    "        conditions = []\n",
    "        symptoms = []\n",
    "        \n",
    "        for entity in ner_results:\n",
    "            entity_type = entity['entity_group']\n",
    "            entity_word = entity['word'].strip()\n",
    "            \n",
    "            # Clean up the entity word - remove hashtags and extra spaces\n",
    "            entity_word = entity_word.replace('#', '').replace('##', '')\n",
    "            \n",
    "            # Only keep meaningful terms (length > 2 characters)\n",
    "            if len(entity_word) > 2:\n",
    "                if entity_type == \"Medication\":\n",
    "                    medications.append(entity_word)\n",
    "                elif entity_type in [\"Disease\", \"Disease_disorder\"]:\n",
    "                    conditions.append(entity_word)\n",
    "                elif entity_type == \"Symptom\":\n",
    "                    symptoms.append(entity_word)\n",
    "        \n",
    "        # Deduplicate and format\n",
    "        result[\"medications_prescribed\"] = \", \".join(sorted(set(medications))) if medications else \"None documented\"\n",
    "        result[\"conditions_identified\"] = \", \".join(sorted(set(conditions))) if conditions else \"None documented\"\n",
    "        result[\"symptoms_noted\"] = \", \".join(sorted(set(symptoms))) if symptoms else \"None documented\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] NER failed for patient {patient_id}: {str(e)}\")\n",
    "        result[\"medications_prescribed\"] = \"Error extracting medications\"\n",
    "        result[\"conditions_identified\"] = \"Error extracting conditions\"\n",
    "        result[\"symptoms_noted\"] = \"Error extracting symptoms\"\n",
    "    \n",
    "    # --- STEP 2: Extract Doctor Instructions ---\n",
    "    try:\n",
    "        # Look for instruction patterns in the text\n",
    "        instruction_patterns = [\n",
    "            r\"(?:plan|instructions?|advice|prescribed|ordered|started on|continue)[:\\s]+([^.|]+)\",\n",
    "            r\"(?:patient (?:should|advised to|instructed to))[:\\s]+([^.|]+)\",\n",
    "            r\"(?:discharge (?:instructions?|plan))[:\\s]+([^.|]+)\"\n",
    "        ]\n",
    "        \n",
    "        instructions = []\n",
    "        for pattern in instruction_patterns:\n",
    "            matches = re.findall(pattern, full_text, re.IGNORECASE)\n",
    "            instructions.extend([match.strip() for match in matches if len(match.strip()) > 10])\n",
    "        \n",
    "        if instructions:\n",
    "            # Deduplicate and limit to top 3 most relevant\n",
    "            unique_instructions = list(set(instructions))[:3]\n",
    "            result[\"doctor_instructions\"] = \" | \".join(unique_instructions)\n",
    "        else:\n",
    "            result[\"doctor_instructions\"] = \"Please consult your discharge paperwork for detailed instructions\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Instruction extraction failed for patient {patient_id}: {str(e)}\")\n",
    "        result[\"doctor_instructions\"] = \"Please consult your discharge paperwork\"\n",
    "    \n",
    "    # --- STEP 3: Extract Follow-up Actions ---\n",
    "    try:\n",
    "        # Look for follow-up patterns\n",
    "        followup_patterns = [\n",
    "            r\"(?:follow[- ]?up)[:\\s]+([^.|]+)\",\n",
    "            r\"(?:return to|see|consult|appointment with)[:\\s]+([^.|]+)\",\n",
    "            r\"(?:scheduled for|referred to)[:\\s]+([^.|]+)\"\n",
    "        ]\n",
    "        \n",
    "        followups = []\n",
    "        for pattern in followup_patterns:\n",
    "            matches = re.findall(pattern, full_text, re.IGNORECASE)\n",
    "            followups.extend([match.strip() for match in matches if len(match.strip()) > 5])\n",
    "        \n",
    "        if followups:\n",
    "            unique_followups = list(set(followups))[:3]\n",
    "            result[\"follow_up_actions\"] = \" | \".join(unique_followups)\n",
    "        else:\n",
    "            result[\"follow_up_actions\"] = \"No specific follow-up documented. Contact your provider if symptoms worsen.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Follow-up extraction failed for patient {patient_id}: {str(e)}\")\n",
    "        result[\"follow_up_actions\"] = \"Please review your discharge instructions\"\n",
    "    \n",
    "    patient_portal_data.append(result)\n",
    "    print(f\"  ✓ Completed processing for Patient ID: {patient_id}\")\n",
    "\n",
    "# --- 5. Create DataFrame with Proper Schema ---\n",
    "schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"patient_name\", StringType(), True),\n",
    "    StructField(\"medications_prescribed\", StringType(), True),\n",
    "    StructField(\"conditions_identified\", StringType(), True),\n",
    "    StructField(\"symptoms_noted\", StringType(), True),\n",
    "    StructField(\"doctor_instructions\", StringType(), True),\n",
    "    StructField(\"follow_up_actions\", StringType(), True)\n",
    "])\n",
    "\n",
    "patient_portal_df = spark.createDataFrame(patient_portal_data, schema=schema)\n",
    "\n",
    "# --- 6. Display Results ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATIENT PORTAL - FINAL OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "display(patient_portal_df)\n",
    "patient_portal_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"patient_portal_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80864aa3-1e3c-49b8-852b-e848403ffd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- SAMPLE OUTPUT FOR ONE PATIENT ---\n\nPatient: John Doe (ID: 101)\n\n\uD83D\uDCCB Medications Prescribed:\n   None documented\n\n\uD83C\uDFE5 Conditions Identified:\n   None documented\n\n\uD83E\uDD12 Symptoms Noted:\n   None documented\n\n\uD83D\uDC8A Doctor's Instructions:\n   ---\n\nFamily anxious | send wood culture, start bankomacinal, consult surgery\n\n\uD83D\uDCC5 Follow-up Actions:\n   surgery\n\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Show sample for one patient ---\n",
    "print(\"\\n--- SAMPLE OUTPUT FOR ONE PATIENT ---\")\n",
    "sample = patient_portal_df.limit(1).collect()[0]\n",
    "print(f\"\\nPatient: {sample['patient_name']} (ID: {sample['patient_id']})\")\n",
    "print(f\"\\n\uD83D\uDCCB Medications Prescribed:\\n   {sample['medications_prescribed']}\")\n",
    "print(f\"\\n\uD83C\uDFE5 Conditions Identified:\\n   {sample['conditions_identified']}\")\n",
    "print(f\"\\n\uD83E\uDD12 Symptoms Noted:\\n   {sample['symptoms_noted']}\")\n",
    "print(f\"\\n\uD83D\uDC8A Doctor's Instructions:\\n   {sample['doctor_instructions']}\")\n",
    "print(f\"\\n\uD83D\uDCC5 Follow-up Actions:\\n   {sample['follow_up_actions']}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f226520-3919-491f-b772-1aa6cf305a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nUSE CASE 4: HOSPITAL READMISSION RISK PREDICTION\n================================================================================\n\n--- Step 1: Loading and Engineering Features ---\n\n--- Step 2: Joining All Features ---\nFeature Engineering Complete. Sample:\n+----------+------------+---------+-------------+----------------+---------------+-------------------+------------+---------------------+---------------------+-----------------+\n|patient_id|patient_name|age_group|num_lab_tests|num_high_results|num_low_results|medication_mentions|notes_length|has_severity_keywords|has_positive_keywords|readmitted_30days|\n+----------+------------+---------+-------------+----------------+---------------+-------------------+------------+---------------------+---------------------+-----------------+\n|101       |John Doe    |senior   |3            |1               |0              |1                  |116         |1                    |1                    |0                |\n|102       |Jane Smith  |senior   |2            |0               |1              |1                  |134         |1                    |1                    |1                |\n|103       |Robert Brown|senior   |3            |0               |1              |1                  |127         |0                    |0                    |0                |\n|104       |Maria Garcia|senior   |3            |1               |0              |2                  |105         |0                    |0                    |1                |\n|105       |David Lee   |senior   |2            |0               |0              |1                  |135         |0                    |0                    |0                |\n+----------+------------+---------+-------------+----------------+---------------+-------------------+------------+---------------------+---------------------+-----------------+\nonly showing top 5 rows\n\n--- Step 3: Preparing Data for Machine Learning ---\nTraining set size: 16\nTest set size: 3\n\n--- Step 4: Training Logistic Regression Model ---\n\n--- Logistic Regression Predictions ---\n+----------+------------+-----------------+----------+----------------------------------------+\n|patient_id|patient_name|readmitted_30days|prediction|probability                             |\n+----------+------------+-----------------+----------+----------------------------------------+\n|109       |Kevin Patel |0                |0.0       |[0.6729080878931577,0.3270919121068423] |\n|202       |Ranjan      |0                |0.0       |[0.8855717310040646,0.11442826899593539]|\n|205       |Ramana      |1                |0.0       |[0.9149627005505293,0.0850372994494707] |\n+----------+------------+-----------------+----------+----------------------------------------+\n\n\n--- Step 5: Training Random Forest Model ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7925695283119308>, line 168\u001B[0m\n",
       "\u001B[1;32m    165\u001B[0m pipeline_rf \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39m[indexer, assembler, scaler, rf])\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n",
       "\u001B[0;32m--> 168\u001B[0m model_rf \u001B[38;5;241m=\u001B[39m pipeline_rf\u001B[38;5;241m.\u001B[39mfit(train_df)\n",
       "\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n",
       "\u001B[1;32m    171\u001B[0m predictions_rf \u001B[38;5;241m=\u001B[39m model_rf\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n",
       "\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n",
       "\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n",
       "\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n",
       "\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n",
       "\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n",
       "\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    319\u001B[0m     )\n",
       "\u001B[1;32m    320\u001B[0m )\n",
       "\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n",
       "\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n",
       "\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: [CONNECT_ML.MODEL_SIZE_OVERFLOW_EXCEPTION] Generic Spark Connect ML error. The fitted or loaded model size is about 114564040 bytes.\n",
       "Please fit or load a model smaller than 104857600 bytes. SQLSTATE: XX000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.connect.ml.MLModelSizeOverflowException\n",
       "\tat org.apache.spark.sql.connect.ml.MLCache.checkModelSize(MLCache.scala:106)\n",
       "\tat org.apache.spark.sql.connect.ml.MLCache.register(MLCache.scala:154)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:264)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:551)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:604)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:580)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:604)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3329)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3314)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "[CONNECT_ML.MODEL_SIZE_OVERFLOW_EXCEPTION] Generic Spark Connect ML error. The fitted or loaded model size is about 114564040 bytes.\nPlease fit or load a model smaller than 104857600 bytes. SQLSTATE: XX000\n\nJVM stacktrace:\norg.apache.spark.sql.connect.ml.MLModelSizeOverflowException\n\tat org.apache.spark.sql.connect.ml.MLCache.checkModelSize(MLCache.scala:106)\n\tat org.apache.spark.sql.connect.ml.MLCache.register(MLCache.scala:154)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:264)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:551)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:580)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3329)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       },
       "metadata": {
        "errorSummary": "[CONNECT_ML.MODEL_SIZE_OVERFLOW_EXCEPTION] Generic Spark Connect ML error. The fitted or loaded model size is about 114564040 bytes.\nPlease fit or load a model smaller than 104857600 bytes. SQLSTATE: XX000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "CONNECT_ML.MODEL_SIZE_OVERFLOW_EXCEPTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XX000",
        "stackTrace": "org.apache.spark.sql.connect.ml.MLModelSizeOverflowException\n\tat org.apache.spark.sql.connect.ml.MLCache.checkModelSize(MLCache.scala:106)\n\tat org.apache.spark.sql.connect.ml.MLCache.register(MLCache.scala:154)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:264)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:551)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:580)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3329)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-7925695283119308>, line 168\u001B[0m\n\u001B[1;32m    165\u001B[0m pipeline_rf \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39m[indexer, assembler, scaler, rf])\n\u001B[1;32m    167\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m model_rf \u001B[38;5;241m=\u001B[39m pipeline_rf\u001B[38;5;241m.\u001B[39mfit(train_df)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[1;32m    171\u001B[0m predictions_rf \u001B[38;5;241m=\u001B[39m model_rf\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    319\u001B[0m     )\n\u001B[1;32m    320\u001B[0m )\n\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: [CONNECT_ML.MODEL_SIZE_OVERFLOW_EXCEPTION] Generic Spark Connect ML error. The fitted or loaded model size is about 114564040 bytes.\nPlease fit or load a model smaller than 104857600 bytes. SQLSTATE: XX000\n\nJVM stacktrace:\norg.apache.spark.sql.connect.ml.MLModelSizeOverflowException\n\tat org.apache.spark.sql.connect.ml.MLCache.checkModelSize(MLCache.scala:106)\n\tat org.apache.spark.sql.connect.ml.MLCache.register(MLCache.scala:154)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:264)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:551)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:580)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:604)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3329)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "USE CASE 4: HOSPITAL READMISSION RISK PREDICTION\n",
    "================================================\n",
    "Predict the probability of a patient being readmitted within 30 days\n",
    "using Spark ML (Logistic Regression & Random Forest)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"USE CASE 4: HOSPITAL READMISSION RISK PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 1: FEATURE ENGINEERING\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 1: Loading and Engineering Features ---\")\n",
    "\n",
    "# Load your existing tables\n",
    "lab_results_df = spark.read.table(\"lab_results_silver\")\n",
    "patient_notes_df = spark.read.table(\"patient_notes\")\n",
    "discharge_summary_df = spark.read.table(\"discharge_summary_gold\")\n",
    "\n",
    "# --- Feature 1: Count of Lab Tests per Patient ---\n",
    "lab_test_counts = lab_results_df.groupBy(\"patient_id\").agg(\n",
    "    F.count(\"test\").alias(\"num_lab_tests\"),\n",
    "    F.sum(F.when(F.lower(F.col(\"result\")).contains(\"high\"), 1).otherwise(0)).alias(\"num_high_results\"),\n",
    "    F.sum(F.when(F.lower(F.col(\"result\")).contains(\"low\"), 1).otherwise(0)).alias(\"num_low_results\")\n",
    ")\n",
    "\n",
    "# --- Feature 2: Extract Age from Patient Name (if available) or create synthetic ---\n",
    "# For demonstration, let's create age groups based on patient_id patterns\n",
    "patient_features = patient_notes_df.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\"\n",
    ").withColumn(\n",
    "    \"age_group\",\n",
    "    F.when(F.col(\"patient_id\").cast(\"int\") <= 105, \"senior\")  # 101-105\n",
    "     .when(F.col(\"patient_id\").cast(\"int\") <= 110, \"adult\")   # 106-110\n",
    "     .otherwise(\"middle_aged\")  # 201+\n",
    ")\n",
    "\n",
    "# --- Feature 3: Extract Medication Count from Discharge Summary ---\n",
    "medications_count = discharge_summary_df.select(\n",
    "    \"patient_id\",\n",
    "    F.size(F.split(F.col(\"Full_History_Text\"), \"medication|prescribed|started on\")).alias(\"medication_mentions\")\n",
    ")\n",
    "\n",
    "# --- Feature 4: Severity Score from Nursing Notes ---\n",
    "severity_features = patient_notes_df.select(\n",
    "    \"patient_id\",\n",
    "    F.length(\"nursing_notes\").alias(\"notes_length\"),\n",
    "    # Count severity indicators\n",
    "    (F.lower(F.col(\"nursing_notes\")).rlike(\"pain|distress|critical|severe|urgent\")).cast(\"int\").alias(\"has_severity_keywords\"),\n",
    "    # Count positive indicators\n",
    "    (F.lower(F.col(\"nursing_notes\")).rlike(\"stable|improved|comfortable|recovering\")).cast(\"int\").alias(\"has_positive_keywords\")\n",
    ")\n",
    "\n",
    "# --- Feature 5: CREATE SYNTHETIC READMISSION LABEL ---\n",
    "# In real scenarios, this would come from actual readmission data\n",
    "# For demonstration, we'll create labels based on risk factors\n",
    "readmission_labels = patient_notes_df.select(\"patient_id\").withColumn(\n",
    "    \"readmitted_30days\",\n",
    "    # Patients with IDs 102, 104, 107, 201, 205 are \"readmitted\" for demo\n",
    "    F.when(F.col(\"patient_id\").isin([\"102\", \"104\", \"107\", \"201\", \"205\"]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# --- STEP 2: JOIN ALL FEATURES ---\n",
    "print(\"\\n--- Step 2: Joining All Features ---\")\n",
    "\n",
    "ml_dataset = patient_features \\\n",
    "    .join(lab_test_counts, \"patient_id\", \"left\") \\\n",
    "    .join(medications_count, \"patient_id\", \"left\") \\\n",
    "    .join(severity_features, \"patient_id\", \"left\") \\\n",
    "    .join(readmission_labels, \"patient_id\", \"left\") \\\n",
    "    .fillna(0)  # Fill nulls with 0\n",
    "\n",
    "print(\"Feature Engineering Complete. Sample:\")\n",
    "ml_dataset.show(5, truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 3: PREPARE DATA FOR ML\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 3: Preparing Data for Machine Learning ---\")\n",
    "\n",
    "# Convert categorical feature (age_group) to numeric\n",
    "indexer = StringIndexer(inputCol=\"age_group\", outputCol=\"age_group_indexed\")\n",
    "\n",
    "# Select numeric features\n",
    "feature_cols = [\n",
    "    \"age_group_indexed\",\n",
    "    \"num_lab_tests\",\n",
    "    \"num_high_results\",\n",
    "    \"num_low_results\",\n",
    "    \"medication_mentions\",\n",
    "    \"notes_length\",\n",
    "    \"has_severity_keywords\",\n",
    "    \"has_positive_keywords\"\n",
    "]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "\n",
    "# Scale features (important for logistic regression)\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# Split data into training and test sets (80/20)\n",
    "train_df, test_df = ml_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 4: TRAIN LOGISTIC REGRESSION MODEL\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 4: Training Logistic Regression Model ---\")\n",
    "\n",
    "# Create Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_lr = Pipeline(stages=[indexer, assembler, scaler, lr])\n",
    "\n",
    "# Train model\n",
    "model_lr = pipeline_lr.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions_lr = model_lr.transform(test_df)\n",
    "\n",
    "print(\"\\n--- Logistic Regression Predictions ---\")\n",
    "predictions_lr.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\",\n",
    "    \"readmitted_30days\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 5: TRAIN RANDOM FOREST MODEL\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 5: Training Random Forest Model ---\")\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_rf = Pipeline(stages=[indexer, assembler, scaler, rf])\n",
    "\n",
    "# Train model\n",
    "model_rf = pipeline_rf.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions_rf = model_rf.transform(test_df)\n",
    "\n",
    "print(\"\\n--- Random Forest Predictions ---\")\n",
    "predictions_rf.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\",\n",
    "    \"readmitted_30days\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 6: EVALUATE MODELS\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 6: Model Evaluation ---\")\n",
    "\n",
    "# Binary Classification Evaluator (for AUC-ROC)\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Multiclass Evaluator (for accuracy)\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_auc = evaluator_auc.evaluate(predictions_lr)\n",
    "lr_accuracy = evaluator_acc.evaluate(predictions_lr)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA LOGISTIC REGRESSION PERFORMANCE:\")\n",
    "print(f\"   AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"   Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_auc = evaluator_auc.evaluate(predictions_rf)\n",
    "rf_accuracy = evaluator_acc.evaluate(predictions_rf)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA RANDOM FOREST PERFORMANCE:\")\n",
    "print(f\"   AUC-ROC: {rf_auc:.4f}\")\n",
    "print(f\"   Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 7: CREATE FINAL RISK PREDICTION TABLE\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 7: Creating Final Risk Prediction Table ---\")\n",
    "\n",
    "# Use the best model (let's use Random Forest for this example)\n",
    "final_predictions = model_rf.transform(ml_dataset)\n",
    "\n",
    "# Extract probability of readmission (probability vector's second element)\n",
    "readmission_risk_df = final_predictions.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\",\n",
    "    F.col(\"prediction\").cast(\"int\").alias(\"predicted_readmission\"),\n",
    "    F.col(\"probability\").getItem(1).alias(\"readmission_probability\")\n",
    ").withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"readmission_probability\") >= 0.7, \"HIGH RISK\")\n",
    "     .when(F.col(\"readmission_probability\") >= 0.4, \"MEDIUM RISK\")\n",
    "     .otherwise(\"LOW RISK\")\n",
    ").orderBy(F.col(\"readmission_probability\").desc())\n",
    "\n",
    "print(\"\\n--- FINAL READMISSION RISK PREDICTIONS ---\")\n",
    "readmission_risk_df.show(truncate=False)\n",
    "\n",
    "# Save to Delta table\n",
    "readmission_risk_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"readmission_risk_predictions\")\n",
    "\n",
    "print(\"\\n✓ Readmission risk predictions saved to table: readmission_risk_predictions\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 8: FEATURE IMPORTANCE (Random Forest)\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 8: Feature Importance Analysis ---\")\n",
    "\n",
    "# Extract the Random Forest model from the pipeline\n",
    "rf_model = model_rf.stages[-1]\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = list(zip(feature_cols, rf_model.featureImportances.toArray()))\n",
    "feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA TOP FEATURES PREDICTING READMISSION:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance_sorted, 1):\n",
    "    print(f\"   {i}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 9: SUMMARY STATISTICS\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READMISSION RISK PREDICTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_stats = readmission_risk_df.groupBy(\"risk_category\").agg(\n",
    "    F.count(\"*\").alias(\"num_patients\"),\n",
    "    F.avg(\"readmission_probability\").alias(\"avg_risk_score\")\n",
    ").orderBy(F.desc(\"avg_risk_score\"))\n",
    "\n",
    "summary_stats.show()\n",
    "\n",
    "print(\"\\n✓ Use Case 4 Complete: Hospital Readmission Risk Prediction\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b7f5d3-2ffe-4e96-8b56-514e574bb60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "HOSPITAL READMISSION PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8385913-740a-49b9-9e08-7935cd7f1069",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4. HOSPITAL READMISSION PREDICTION"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nUSE CASE 4: HOSPITAL READMISSION RISK PREDICTION\nUsing Existing Pipeline Tables Only - No Reprocessing!\n================================================================================\n\n--- Step 1: Loading Existing Delta Tables ---\n✓ Loaded transcripts_bronze: 20 records\n✓ Loaded lab_results_silver: 31 records\n✓ Loaded nursing_notes_silver: 19 records\n✓ Loaded discharge_summary_gold: 19 records\n✓ Loaded patient_portal_gold: 19 records\n\n--- Step 2: Feature Engineering from Existing Tables ---\nLab Features:\n+----------+---------------+------------+-------------------+------------------+-------------------+\n|patient_id|total_lab_tests|unique_tests|abnormal_high_count|abnormal_low_count|missing_dates_count|\n+----------+---------------+------------+-------------------+------------------+-------------------+\n|101       |3              |3           |2                  |0                 |0                  |\n|102       |2              |2           |0                  |1                 |0                  |\n|103       |3              |3           |0                  |1                 |0                  |\n|104       |3              |3           |1                  |0                 |0                  |\n|105       |2              |2           |0                  |0                 |0                  |\n+----------+---------------+------------+-------------------+------------------+-------------------+\nonly showing top 5 rows\n\nNursing Notes Features:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7925695283119309>, line 69\u001B[0m\n",
       "\u001B[1;32m     55\u001B[0m nursing_features \u001B[38;5;241m=\u001B[39m nursing_notes_silver\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     56\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     57\u001B[0m     F\u001B[38;5;241m.\u001B[39mlength(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnursing_notes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotes_length\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     65\u001B[0m     (F\u001B[38;5;241m.\u001B[39msize(F\u001B[38;5;241m.\u001B[39msplit(F\u001B[38;5;241m.\u001B[39mlower(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnursing_notes\u001B[39m\u001B[38;5;124m\"\u001B[39m)), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvital\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvital_signs_mentions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     66\u001B[0m )\n",
       "\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mNursing Notes Features:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 69\u001B[0m nursing_features\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m, truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# --- Feature Set 3: Patient Portal Medications ---\u001B[39;00m\n",
       "\u001B[1;32m     72\u001B[0m medications_features \u001B[38;5;241m=\u001B[39m patient_portal_gold\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     73\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# Count number of medications\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     81\u001B[0m     (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfollow_up_actions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mrlike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsult|appointment|follow\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_followup\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     82\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    865\u001B[0m             },\n",
       "\u001B[1;32m    866\u001B[0m         )\n",
       "\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n",
       "\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n",
       "\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n",
       "\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n",
       "\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n",
       "\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n",
       "\u001B[1;32m    874\u001B[0m     ),\n",
       "\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n",
       "\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n",
       "\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m   1892\u001B[0m     )\n",
       "\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n",
       "\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n",
       "'Project [patient_id#18064L, 'length('nursing_notes) AS notes_length#18054, cast('rlike('lower('nursing_notes), pain|distress|critical|severe|acute|emergency) as int) AS severity_mentioned#18055, cast('rlike('lower('nursing_notes), infection|complication|deteriorat|worsen) as int) AS complications_mentioned#18056, cast('rlike('lower('nursing_notes), stable|improving|improved|recovering|better) as int) AS positive_progress#18057, '`-`('size('split('lower('nursing_notes), vital, -1)), 1) AS vital_signs_mentions#18058]\n",
       "+- SubqueryAlias workspace.default.nursing_notes_silver\n",
       "   +- Relation workspace.default.nursing_notes_silver[patient_id#18064L,patient_problem#18065,vitals#18066,assessment#18067,plan#18068] parquet\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n",
       "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
       "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n",
       "\tat scala.Option.flatMap(Option.scala:283)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n'Project [patient_id#18064L, 'length('nursing_notes) AS notes_length#18054, cast('rlike('lower('nursing_notes), pain|distress|critical|severe|acute|emergency) as int) AS severity_mentioned#18055, cast('rlike('lower('nursing_notes), infection|complication|deteriorat|worsen) as int) AS complications_mentioned#18056, cast('rlike('lower('nursing_notes), stable|improving|improved|recovering|better) as int) AS positive_progress#18057, '`-`('size('split('lower('nursing_notes), vital, -1)), 1) AS vital_signs_mentions#18058]\n+- SubqueryAlias workspace.default.nursing_notes_silver\n   +- Relation workspace.default.nursing_notes_silver[patient_id#18064L,patient_problem#18065,vitals#18066,assessment#18067,plan#18068] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703\n== DataFrame ==\n\"col\" was called from <command-7925695283119309>, line 57 in cell [274]\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": "<command-7925695283119309>, line 57 in cell [274]",
        "pysparkFragment": "col",
        "pysparkSummary": "",
        "sqlState": "42703",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7925695283119309>, line 69\u001B[0m\n\u001B[1;32m     55\u001B[0m nursing_features \u001B[38;5;241m=\u001B[39m nursing_notes_silver\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     57\u001B[0m     F\u001B[38;5;241m.\u001B[39mlength(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnursing_notes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotes_length\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     65\u001B[0m     (F\u001B[38;5;241m.\u001B[39msize(F\u001B[38;5;241m.\u001B[39msplit(F\u001B[38;5;241m.\u001B[39mlower(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnursing_notes\u001B[39m\u001B[38;5;124m\"\u001B[39m)), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvital\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvital_signs_mentions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     66\u001B[0m )\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mNursing Notes Features:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 69\u001B[0m nursing_features\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m, truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# --- Feature Set 3: Patient Portal Medications ---\u001B[39;00m\n\u001B[1;32m     72\u001B[0m medications_features \u001B[38;5;241m=\u001B[39m patient_portal_gold\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# Count number of medications\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     81\u001B[0m     (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfollow_up_actions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mrlike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsult|appointment|follow\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_followup\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     82\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    865\u001B[0m             },\n\u001B[1;32m    866\u001B[0m         )\n\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n\u001B[1;32m    874\u001B[0m     ),\n\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m   1892\u001B[0m     )\n\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n'Project [patient_id#18064L, 'length('nursing_notes) AS notes_length#18054, cast('rlike('lower('nursing_notes), pain|distress|critical|severe|acute|emergency) as int) AS severity_mentioned#18055, cast('rlike('lower('nursing_notes), infection|complication|deteriorat|worsen) as int) AS complications_mentioned#18056, cast('rlike('lower('nursing_notes), stable|improving|improved|recovering|better) as int) AS positive_progress#18057, '`-`('size('split('lower('nursing_notes), vital, -1)), 1) AS vital_signs_mentions#18058]\n+- SubqueryAlias workspace.default.nursing_notes_silver\n   +- Relation workspace.default.nursing_notes_silver[patient_id#18064L,patient_problem#18065,vitals#18066,assessment#18067,plan#18068] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "USE CASE 4: HOSPITAL READMISSION RISK PREDICTION (Using Existing Pipeline Tables)\n",
    "==================================================================================\n",
    "Predict the probability of a patient being readmitted within 30 days\n",
    "using Spark ML - ONLY reading from existing Delta tables\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"USE CASE 4: HOSPITAL READMISSION RISK PREDICTION\")\n",
    "print(\"Using Existing Pipeline Tables Only - No Reprocessing!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 1: LOAD EXISTING DELTA TABLES\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 1: Loading Existing Delta Tables ---\")\n",
    "\n",
    "transcripts_bronze = spark.read.table(\"transcripts_bronze\")\n",
    "lab_results_silver = spark.read.table(\"lab_results_silver\")\n",
    "nursing_notes_silver = spark.read.table(\"nursing_notes_silver\")\n",
    "discharge_summary_gold = spark.read.table(\"discharge_summary_gold\")\n",
    "patient_portal_gold = spark.read.table(\"patient_portal_gold\")\n",
    "\n",
    "print(f\"✓ Loaded transcripts_bronze: {transcripts_bronze.count()} records\")\n",
    "print(f\"✓ Loaded lab_results_silver: {lab_results_silver.count()} records\")\n",
    "print(f\"✓ Loaded nursing_notes_silver: {nursing_notes_silver.count()} records\")\n",
    "print(f\"✓ Loaded discharge_summary_gold: {discharge_summary_gold.count()} records\")\n",
    "print(f\"✓ Loaded patient_portal_gold: {patient_portal_gold.count()} records\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 2: FEATURE ENGINEERING FROM EXISTING TABLES\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 2: Feature Engineering from Existing Tables ---\")\n",
    "\n",
    "# --- Feature Set 1: Lab Results Analysis ---\n",
    "lab_features = lab_results_silver.groupBy(\"patient_id\").agg(\n",
    "    F.count(\"test\").alias(\"total_lab_tests\"),\n",
    "    F.countDistinct(\"test\").alias(\"unique_tests\"),\n",
    "    F.sum(F.when(F.lower(F.col(\"result\")).rlike(\"high|elevated|abnormal\"), 1).otherwise(0)).alias(\"abnormal_high_count\"),\n",
    "    F.sum(F.when(F.lower(F.col(\"result\")).rlike(\"low|decreased\"), 1).otherwise(0)).alias(\"abnormal_low_count\"),\n",
    "    F.sum(F.when(F.col(\"date\") == \"NA\", 1).otherwise(0)).alias(\"missing_dates_count\")\n",
    ")\n",
    "\n",
    "print(\"Lab Features:\")\n",
    "lab_features.show(5, truncate=False)\n",
    "\n",
    "# --- Feature Set 2: Nursing Notes Complexity ---\n",
    "nursing_features = nursing_notes_silver.select(\n",
    "    \"patient_id\",\n",
    "    F.length(\"nursing_notes\").alias(\"notes_length\"),\n",
    "    # Severity indicators\n",
    "    (F.lower(F.col(\"nursing_notes\")).rlike(\"pain|distress|critical|severe|acute|emergency\")).cast(\"int\").alias(\"severity_mentioned\"),\n",
    "    # Complication indicators\n",
    "    (F.lower(F.col(\"nursing_notes\")).rlike(\"infection|complication|deteriorat|worsen\")).cast(\"int\").alias(\"complications_mentioned\"),\n",
    "    # Positive progress indicators\n",
    "    (F.lower(F.col(\"nursing_notes\")).rlike(\"stable|improving|improved|recovering|better\")).cast(\"int\").alias(\"positive_progress\"),\n",
    "    # Count vital signs mentions (indicator of monitoring intensity)\n",
    "    (F.size(F.split(F.lower(F.col(\"nursing_notes\")), \"vital\")) - 1).alias(\"vital_signs_mentions\")\n",
    ")\n",
    "\n",
    "print(\"\\nNursing Notes Features:\")\n",
    "nursing_features.show(5, truncate=False)\n",
    "\n",
    "# --- Feature Set 3: Patient Portal Medications ---\n",
    "medications_features = patient_portal_gold.select(\n",
    "    \"patient_id\",\n",
    "    # Count number of medications\n",
    "    F.when(F.col(\"medications_prescribed\") == \"None documented\", 0)\n",
    "     .otherwise(F.size(F.split(F.col(\"medications_prescribed\"), \",\"))).alias(\"medication_count\"),\n",
    "    # Count number of conditions\n",
    "    F.when(F.col(\"conditions_identified\") == \"None documented\", 0)\n",
    "     .otherwise(F.size(F.split(F.col(\"conditions_identified\"), \",\"))).alias(\"condition_count\"),\n",
    "    # Check if follow-up exists\n",
    "    (F.col(\"follow_up_actions\").rlike(\"consult|appointment|follow\")).cast(\"int\").alias(\"has_followup\")\n",
    ")\n",
    "\n",
    "print(\"\\nMedication & Condition Features:\")\n",
    "medications_features.show(5, truncate=False)\n",
    "\n",
    "# --- Feature Set 4: Discharge Summary Complexity ---\n",
    "discharge_features = discharge_summary_gold.select(\n",
    "    \"patient_id\",\n",
    "    F.length(\"Full_History_Text\").alias(\"discharge_text_length\"),\n",
    "    # Count medical terms (indicator of case complexity)\n",
    "    (F.size(F.split(F.lower(F.col(\"Full_History_Text\")), \"diagnosis|treatment|procedure\")) - 1).alias(\"medical_terms_count\")\n",
    ")\n",
    "\n",
    "print(\"\\nDischarge Summary Features:\")\n",
    "discharge_features.show(5, truncate=False)\n",
    "\n",
    "# --- Feature Set 5: Transcript Indicators ---\n",
    "transcript_features = transcripts_bronze.select(\n",
    "    \"patient_id\",\n",
    "    F.length(\"transcript_text\").alias(\"transcript_length\"),\n",
    "    # Detect emotional distress in audio\n",
    "    (F.lower(F.col(\"transcript_text\")).rlike(\"worried|scared|anxious|concerned|afraid\")).cast(\"int\").alias(\"emotional_distress\"),\n",
    "    # Detect comprehension issues\n",
    "    (F.lower(F.col(\"transcript_text\")).rlike(\"don't understand|confused|unclear|repeat\")).cast(\"int\").alias(\"comprehension_issues\")\n",
    ")\n",
    "\n",
    "print(\"\\nTranscript Features:\")\n",
    "transcript_features.show(5, truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 3: JOIN ALL FEATURES\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 3: Joining All Features into Master Table ---\")\n",
    "\n",
    "ml_dataset = lab_features \\\n",
    "    .join(nursing_features, \"patient_id\", \"left\") \\\n",
    "    .join(medications_features, \"patient_id\", \"left\") \\\n",
    "    .join(discharge_features, \"patient_id\", \"left\") \\\n",
    "    .join(transcript_features, \"patient_id\", \"left\") \\\n",
    "    .fillna(0)  # Fill any remaining nulls with 0\n",
    "\n",
    "# Add patient name for reference\n",
    "ml_dataset = ml_dataset.join(\n",
    "    patient_portal_gold.select(\"patient_id\", \"patient_name\"),\n",
    "    \"patient_id\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nMaster Feature Table:\")\n",
    "ml_dataset.show(5, truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 4: CREATE SYNTHETIC READMISSION LABELS\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 4: Creating Readmission Labels (Synthetic for Demo) ---\")\n",
    "\n",
    "# In production, this would come from actual readmission data\n",
    "# For demo: patients with high risk factors = readmitted\n",
    "ml_dataset = ml_dataset.withColumn(\n",
    "    \"readmitted_30days\",\n",
    "    F.when(\n",
    "        (F.col(\"abnormal_high_count\") >= 2) |\n",
    "        (F.col(\"complications_mentioned\") == 1) |\n",
    "        (F.col(\"medication_count\") >= 3) |\n",
    "        (F.col(\"severity_mentioned\") == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Show label distribution\n",
    "print(\"\\nReadmission Label Distribution:\")\n",
    "ml_dataset.groupBy(\"readmitted_30days\").count().show()\n",
    "\n",
    "# =========================================================\n",
    "# STEP 5: PREPARE DATA FOR ML\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 5: Preparing Data for Machine Learning ---\")\n",
    "\n",
    "# Select feature columns (all numeric)\n",
    "feature_cols = [\n",
    "    \"total_lab_tests\",\n",
    "    \"unique_tests\",\n",
    "    \"abnormal_high_count\",\n",
    "    \"abnormal_low_count\",\n",
    "    \"missing_dates_count\",\n",
    "    \"notes_length\",\n",
    "    \"severity_mentioned\",\n",
    "    \"complications_mentioned\",\n",
    "    \"positive_progress\",\n",
    "    \"vital_signs_mentions\",\n",
    "    \"medication_count\",\n",
    "    \"condition_count\",\n",
    "    \"has_followup\",\n",
    "    \"discharge_text_length\",\n",
    "    \"medical_terms_count\",\n",
    "    \"transcript_length\",\n",
    "    \"emotional_distress\",\n",
    "    \"comprehension_issues\"\n",
    "]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# Split data (80/20)\n",
    "train_df, test_df = ml_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_df.count()} patients\")\n",
    "print(f\"Test set: {test_df.count()} patients\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 6: TRAIN RANDOM FOREST MODEL\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 6: Training Random Forest Classifier ---\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    numTrees=100,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"✓ Model trained successfully\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 7: MAKE PREDICTIONS\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 7: Making Predictions ---\")\n",
    "\n",
    "# Predict on all data\n",
    "predictions = model.transform(ml_dataset)\n",
    "\n",
    "# Extract readmission probability\n",
    "readmission_predictions = predictions.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\",\n",
    "    \"readmitted_30days\",\n",
    "    F.col(\"prediction\").cast(\"int\").alias(\"predicted_readmission\"),\n",
    "    F.col(\"probability\").getItem(1).alias(\"readmission_risk_score\")\n",
    ").withColumn(\n",
    "    \"risk_category\",\n",
    "    F.when(F.col(\"readmission_risk_score\") >= 0.7, \"\uD83D\uDD34 HIGH RISK\")\n",
    "     .when(F.col(\"readmission_risk_score\") >= 0.4, \"\uD83D\uDFE1 MEDIUM RISK\")\n",
    "     .otherwise(\"\uD83D\uDFE2 LOW RISK\")\n",
    ").orderBy(F.col(\"readmission_risk_score\").desc())\n",
    "\n",
    "print(\"\\n--- READMISSION RISK PREDICTIONS ---\")\n",
    "readmission_predictions.show(20, truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# STEP 8: MODEL EVALUATION\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 8: Model Evaluation ---\")\n",
    "\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "# AUC-ROC\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = evaluator_auc.evaluate(test_predictions)\n",
    "\n",
    "# Accuracy\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"readmitted_30days\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_acc.evaluate(test_predictions)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA MODEL PERFORMANCE:\")\n",
    "print(f\"   AUC-ROC: {auc:.4f}\")\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 9: FEATURE IMPORTANCE\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 9: Feature Importance Analysis ---\")\n",
    "\n",
    "rf_model = model.stages[-1]\n",
    "feature_importance = list(zip(feature_cols, rf_model.featureImportances.toArray()))\n",
    "feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA TOP 10 FEATURES PREDICTING READMISSION:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance_sorted[:10], 1):\n",
    "    print(f\"   {i}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 10: SAVE RESULTS TO GOLD LAYER\n",
    "# =========================================================\n",
    "print(\"\\n--- Step 10: Saving Results to Gold Layer ---\")\n",
    "\n",
    "# Save predictions\n",
    "readmission_predictions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"readmission_risk_gold\")\n",
    "print(\"✓ Saved to table: readmission_risk_gold\")\n",
    "\n",
    "# =========================================================\n",
    "# STEP 11: SUMMARY DASHBOARD\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READMISSION RISK PREDICTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = readmission_predictions.groupBy(\"risk_category\").agg(\n",
    "    F.count(\"*\").alias(\"num_patients\"),\n",
    "    F.avg(\"readmission_risk_score\").alias(\"avg_risk_score\"),\n",
    "    F.min(\"readmission_risk_score\").alias(\"min_risk_score\"),\n",
    "    F.max(\"readmission_risk_score\").alias(\"max_risk_score\")\n",
    ").orderBy(F.desc(\"avg_risk_score\"))\n",
    "\n",
    "summary.display(truncate=False)\n",
    "\n",
    "print(\"\\n✓ Use Case 4 Complete: Hospital Readmission Risk Prediction\")\n",
    "print(\"✓ All data sourced from existing pipeline tables\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b94fbb0-cf8f-42b1-9092-429e5f92a932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "USE CASE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b9b9ef-d4bd-4df6-80e9-617b70a3abb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nINSIGHT 6: DISCHARGE READINESS ASSESSMENT\n================================================================================\n\n\uD83D\uDCCB Discharge Readiness Summary:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7925695283119312>, line 33\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m discharge_readiness \u001B[38;5;241m=\u001B[39m patient_portal_gold\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_name\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m      \u001B[38;5;241m.\u001B[39motherwise(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ NOT READY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     30\u001B[0m )\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDCCB Discharge Readiness Summary:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 33\u001B[0m discharge_readiness\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDC65 Patient Discharge Status:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     36\u001B[0m discharge_readiness\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_readiness_score\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     38\u001B[0m )\u001B[38;5;241m.\u001B[39morderBy(F\u001B[38;5;241m.\u001B[39mdesc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_readiness_score\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    865\u001B[0m             },\n",
       "\u001B[1;32m    866\u001B[0m         )\n",
       "\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n",
       "\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n",
       "\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n",
       "\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n",
       "\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n",
       "\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n",
       "\u001B[1;32m    874\u001B[0m     ),\n",
       "\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n",
       "\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n",
       "\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m   1892\u001B[0m     )\n",
       "\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n",
       "\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n",
       "'Sort ['discharge_status ASC NULLS FIRST], true\n",
       "+- 'Aggregate ['discharge_status], ['discharge_status, count(1) AS count#18320L]\n",
       "   +- 'Project [unresolvedstarwithcolumns(discharge_status, 'when('`>=`('discharge_readiness_score, 3), ✅ READY, '`>=`('discharge_readiness_score, 1), ⏳ MONITOR, ⚠️ NOT READY), Some(List({})))]\n",
       "      +- 'Project [unresolvedstarwithcolumns(discharge_readiness_score, '`+`('`-`('`*`('positive_indicators, 2), 'concerning_indicators), 'has_followup_plan), Some(List({})))]\n",
       "         +- 'Project [unresolvedstarwithcolumns(has_followup_plan, cast('rlike('follow_up_actions, appointment|consult|follow) as int), Some(List({})))]\n",
       "            +- 'Join UsingJoin(Inner, [patient_id])\n",
       "               :- Project [patient_id#18359, patient_name#18360, follow_up_actions#18365]\n",
       "               :  +- SubqueryAlias workspace.default.patient_portal_gold\n",
       "               :     +- Relation workspace.default.patient_portal_gold[patient_id#18359,patient_name#18360,medications_prescribed#18361,conditions_identified#18362,symptoms_noted#18363,doctor_instructions#18364,follow_up_actions#18365] parquet\n",
       "               +- 'Project [patient_id#18366L, cast('rlike('lower('nursing_notes), stable|improved|recovering) as int) AS positive_indicators#18162, cast('rlike('lower('nursing_notes), pain|distress|complication) as int) AS concerning_indicators#18163]\n",
       "                  +- SubqueryAlias workspace.default.nursing_notes_silver\n",
       "                     +- Relation workspace.default.nursing_notes_silver[patient_id#18366L,patient_problem#18367,vitals#18368,assessment#18369,plan#18370] parquet\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n",
       "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
       "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n",
       "\tat scala.Option.flatMap(Option.scala:283)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n'Sort ['discharge_status ASC NULLS FIRST], true\n+- 'Aggregate ['discharge_status], ['discharge_status, count(1) AS count#18320L]\n   +- 'Project [unresolvedstarwithcolumns(discharge_status, 'when('`>=`('discharge_readiness_score, 3), ✅ READY, '`>=`('discharge_readiness_score, 1), ⏳ MONITOR, ⚠️ NOT READY), Some(List({})))]\n      +- 'Project [unresolvedstarwithcolumns(discharge_readiness_score, '`+`('`-`('`*`('positive_indicators, 2), 'concerning_indicators), 'has_followup_plan), Some(List({})))]\n         +- 'Project [unresolvedstarwithcolumns(has_followup_plan, cast('rlike('follow_up_actions, appointment|consult|follow) as int), Some(List({})))]\n            +- 'Join UsingJoin(Inner, [patient_id])\n               :- Project [patient_id#18359, patient_name#18360, follow_up_actions#18365]\n               :  +- SubqueryAlias workspace.default.patient_portal_gold\n               :     +- Relation workspace.default.patient_portal_gold[patient_id#18359,patient_name#18360,medications_prescribed#18361,conditions_identified#18362,symptoms_noted#18363,doctor_instructions#18364,follow_up_actions#18365] parquet\n               +- 'Project [patient_id#18366L, cast('rlike('lower('nursing_notes), stable|improved|recovering) as int) AS positive_indicators#18162, cast('rlike('lower('nursing_notes), pain|distress|complication) as int) AS concerning_indicators#18163]\n                  +- SubqueryAlias workspace.default.nursing_notes_silver\n                     +- Relation workspace.default.nursing_notes_silver[patient_id#18366L,patient_problem#18367,vitals#18368,assessment#18369,plan#18370] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703\n== DataFrame ==\n\"col\" was called from <command-7925695283119312>, line 15 in cell [277]\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": "<command-7925695283119312>, line 15 in cell [277]",
        "pysparkFragment": "col",
        "pysparkSummary": "",
        "sqlState": "42703",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7925695283119312>, line 33\u001B[0m\n\u001B[1;32m      8\u001B[0m discharge_readiness \u001B[38;5;241m=\u001B[39m patient_portal_gold\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_name\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m      \u001B[38;5;241m.\u001B[39motherwise(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ NOT READY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     30\u001B[0m )\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDCCB Discharge Readiness Summary:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m discharge_readiness\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\uD83D\uDC65 Patient Discharge Status:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m discharge_readiness\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_readiness_score\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_status\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     38\u001B[0m )\u001B[38;5;241m.\u001B[39morderBy(F\u001B[38;5;241m.\u001B[39mdesc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdischarge_readiness_score\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    865\u001B[0m             },\n\u001B[1;32m    866\u001B[0m         )\n\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n\u001B[1;32m    874\u001B[0m     ),\n\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m   1892\u001B[0m     )\n\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `nursing_notes` cannot be resolved. Did you mean one of the following? [`assessment`, `patient_id`, `patient_problem`, `plan`, `vitals`]. SQLSTATE: 42703;\n'Sort ['discharge_status ASC NULLS FIRST], true\n+- 'Aggregate ['discharge_status], ['discharge_status, count(1) AS count#18320L]\n   +- 'Project [unresolvedstarwithcolumns(discharge_status, 'when('`>=`('discharge_readiness_score, 3), ✅ READY, '`>=`('discharge_readiness_score, 1), ⏳ MONITOR, ⚠️ NOT READY), Some(List({})))]\n      +- 'Project [unresolvedstarwithcolumns(discharge_readiness_score, '`+`('`-`('`*`('positive_indicators, 2), 'concerning_indicators), 'has_followup_plan), Some(List({})))]\n         +- 'Project [unresolvedstarwithcolumns(has_followup_plan, cast('rlike('follow_up_actions, appointment|consult|follow) as int), Some(List({})))]\n            +- 'Join UsingJoin(Inner, [patient_id])\n               :- Project [patient_id#18359, patient_name#18360, follow_up_actions#18365]\n               :  +- SubqueryAlias workspace.default.patient_portal_gold\n               :     +- Relation workspace.default.patient_portal_gold[patient_id#18359,patient_name#18360,medications_prescribed#18361,conditions_identified#18362,symptoms_noted#18363,doctor_instructions#18364,follow_up_actions#18365] parquet\n               +- 'Project [patient_id#18366L, cast('rlike('lower('nursing_notes), stable|improved|recovering) as int) AS positive_indicators#18162, cast('rlike('lower('nursing_notes), pain|distress|complication) as int) AS concerning_indicators#18163]\n                  +- SubqueryAlias workspace.default.nursing_notes_silver\n                     +- Relation workspace.default.nursing_notes_silver[patient_id#18366L,patient_problem#18367,vitals#18368,assessment#18369,plan#18370] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:618)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1068)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1067)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1067)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1070)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:485)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:485)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:312)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:311)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:266)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:582)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:639)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:639)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:631)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:402)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:629)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:645)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:628)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:626)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# DISCHARGE READINESS INDICATORS\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHT 6: DISCHARGE READINESS ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "discharge_readiness = patient_portal_gold.select(\n",
    "    \"patient_id\",\n",
    "    \"patient_name\",\n",
    "    \"follow_up_actions\"\n",
    ").join(\n",
    "    nursing_notes_silver.select(\n",
    "        \"patient_id\",\n",
    "        (F.lower(F.col(\"nursing_notes\")).rlike(\"stable|improved|recovering\")).cast(\"int\").alias(\"positive_indicators\"),\n",
    "        (F.lower(F.col(\"nursing_notes\")).rlike(\"pain|distress|complication\")).cast(\"int\").alias(\"concerning_indicators\")\n",
    "    ),\n",
    "    \"patient_id\"\n",
    ").withColumn(\n",
    "    \"has_followup_plan\",\n",
    "    (F.col(\"follow_up_actions\").rlike(\"appointment|consult|follow\")).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"discharge_readiness_score\",\n",
    "    (F.col(\"positive_indicators\") * 2) - F.col(\"concerning_indicators\") + F.col(\"has_followup_plan\")\n",
    ").withColumn(\n",
    "    \"discharge_status\",\n",
    "    F.when(F.col(\"discharge_readiness_score\") >= 3, \"✅ READY\")\n",
    "     .when(F.col(\"discharge_readiness_score\") >= 1, \"⏳ MONITOR\")\n",
    "     .otherwise(\"⚠️ NOT READY\")\n",
    ")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB Discharge Readiness Summary:\")\n",
    "discharge_readiness.groupBy(\"discharge_status\").count().orderBy(\"discharge_status\").show()\n",
    "\n",
    "print(\"\\n\uD83D\uDC65 Patient Discharge Status:\")\n",
    "discharge_readiness.select(\n",
    "    \"patient_id\", \"patient_name\", \"discharge_readiness_score\", \"discharge_status\"\n",
    ").orderBy(F.desc(\"discharge_readiness_score\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "744ececf-2457-4b96-b284-965659b017ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "big_data_capstone",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}